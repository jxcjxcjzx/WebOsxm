 
Research Interest 
My research interest include face pose estimation, multi-pose face tracking and head gesture recognition. 
 
Projects 
Head gesture recognition 
Face pose tracking 
Face animation 


Head gesture recognition 
Head gestures, as part of bodily communication, play an important role in human communications. Automatic recognition of head gestures can help understand people's agreement with the speaker, which is useful for human conversation analysis. 
    We address the problem of recognizing multi-pose head nodding and shaking gestures in human conversations. Existing methods mainly recognize head gestures in restricted environments like human robot interaction, where face poses are near frontal and head motions are pre-defined. However, in human conversations, faces of subjects might be in arbitrary poses while head gestures are often subtle. We propose to integrate tracking of face poses and recognition of head gestures in a unified framework, where faces are tracked with a mixed-state particle filter, subtle head motions are detected by the KLT tracker, and head gestures are recognized using a FSM. 
Results

 
Motion direction patterns of nodding and shaking in multi-pose faces. Row (a) are tracked faces where green points are features to estimate motion. Row (b) and row (c) are motion patterns for nodding and shaking gestures, respectively. The number 0-4 in y axis means the motion direction of {NULL, RIGHT, UP, LEFT, DOWN}. Red and blue lines are motions in vertical and horizontal directions, respectively. Nodding gesture in profile and half profile faces might have both periodicities in horizontal and vertical directions. However, for all poses, motion patterns of shaking gestures are similar, although there might be noisy motions in vertical direction like figure in row (c) column (i) 


Head gesture recognition result of a video.The upper figure is the motion pattern in horizontal and vertical directions. The bottom one is the pose tracking result, where red lines are weighted float point poses. The blue and red segments on the bottom mean the recognized head shaking and nodding segments respectively. 


Three representative frames with different poses of this video 


Face pose tracking 
Face poses are important cues to infer people’s visual focus of attention in meetings. 
    We perform face tracking and pose estimation jointly within a mixed-state particle filter framework. Previous methods often used generative appearance models and naive prior state transition. We propose to use discriminating models, Adaboosted face detectors, to both measure observations and provide information for the proposal distribution which is combined with detection responses and prior transition model. Due to pose continuity, faces between discrete poses can be detected by neighboring pose-specific detectors and serve as importance samples. Thus continuous poses are obtained instead of discrete poses. Experiments show that our method is robust to large location and pose changes, partial occlusions and expressions, and can work in real time at 20 fps. 
Results


Tracking results with location and pose changes 

Tracking results of face poses 

 
Tracking results of people in group conversations 

Tracking results of face poses 



JAVA-based 2D Facial Animation for Mobile Devices 
This is a cooperative project between our lab and Siemens Corporate Technology. The goal is to integrate the 2D facial animation techiques into the mobile applications to get an audio-visual synchronized facial animation. The application is based on the MPEG4 face animation framework, where the 2D face models are constructed offline on a PC, and the audio-visual synchronization is generated on the mobile devices when the animation is played. To get better visual effects, a simple 3D face model is employed. The application is developed based on the J2ME framework for platform independency and our face animation engine has been successfully demonstrated on Siemens S65 smartphones. 
Demo
dlg_demo_mobile.avi (2.0MB)

 
 
 

--------------------------------------------------------------------------------
Copyright © 2007-2008, Ligeng DONG
01/30/2007 04:02:47 Last Modified.  
 
