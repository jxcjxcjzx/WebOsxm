


Phinecos(洞庭散人) 

专注于开源技术的研究与应用
 


基于朴素贝叶斯分类器的文本分类算法（上）

  
转载请保留作者信息：
 
作者：phinecos（洞庭散人）
 
Blog：http://phinecos.cnblogs.com/
 
Email：phinecos@163.com
 
 Preface
 
       本文缘起于最近在读的一本书-- Tom M.Mitchell的《机器学习》,书中第6章详细讲解了贝叶斯学习的理论知识，为了将其应用到实际中来，参考了网上许多资料，从而得此文。文章将分为两个部分，第一部分将介绍贝叶斯学习的相关理论(如果你对理论不感兴趣，请直接跳至第二部分<<基于朴素贝叶斯分类器的文本分类算法（下）>>)。第二部分讲如何将贝叶斯分类器应用到中文文本分类，随文附上示例代码。
 
 Introduction
 
我们在《概率论和数理统计》这门课的第一章都学过贝叶斯公式和全概率公式，先来简单复习下：
 
条件概率 

定义 设A, B是两个事件，且P(A)>0 称P(B∣A)=P(AB)/P(A)为在条件A下发生的条件事件B发生的条件概率。
 
乘法公式 设P(A)>0 则有P(AB)=P(B∣A)P(A) 

全概率公式和贝叶斯公式
 
定义 设S为试验E的样本空间，B1, B2, …Bn为E的一组事件，若BiBj=Ф, i≠j, i, j=1, 2, …,n; B1∪B2∪…∪Bn=S则称B1, B2, …, Bn为样本空间的一个划分。
 
定理 设试验E的样本空间为，A为E的事件，B1, B2, …,Bn为的一个划分，且P(Bi)>0 (i=1, 2, …n)，则P(A)=P(A∣B1)P(B1)+P(A∣B2)+ …+P(A∣Bn)P(Bn)称为全概率公式。
 
定理 设试验俄E的样本空间为S，A为E的事件，B1, B2, …,Bn为的一个划分，则
 
P(Bi∣A)=P(A∣Bi)P(Bi)/∑P(B｜Aj)P(Aj)=P(B｜Ai)P(Ai)/P(B) 

称为贝叶斯公式。说明：i，j均为下标，求和均是1到n  
 
 下面我再举个简单的例子来说明下。
 
示例1
 
考虑一个医疗诊断问题，有两种可能的假设：（1）病人有癌症。（2）病人无癌症。样本数据来自某化验测试，它也有两种可能的结果：阳性和阴性。假设我们已经有先验知识：在所有人口中只有0.008的人患病。此外，化验测试对有病的患者有98%的可能返回阳性结果，对无病患者有97%的可能返回阴性结果。
 
上面的数据可以用以下概率式子表示： 

P(cancer)=0.008,P(无cancer)=0.992
 
P(阳性|cancer)=0.98,P(阴性|cancer)=0.02
 
P(阳性|无cancer)=0.03，P(阴性|无cancer)=0.97
 
假设现在有一个新病人，化验测试返回阳性，是否将病人断定为有癌症呢？我们可以来计算极大后验假设：
 
P(阳性|cancer)p(cancer)=0.98*0.008 = 0.0078
 
P(阳性|无cancer)*p(无cancer)=0.03*0.992 = 0.0298
 
因此，应该判断为无癌症。
 
 贝叶斯学习理论
 
       贝叶斯是一种基于概率的学习算法，能够用来计算显式的假设概率，它基于假设的先验概率，给定假设下观察到不同数据的概率以及观察到的数据本身（后面我们可以看到，其实就这么三点东西，呵呵）。
 
      我们用P(h)表示没有训练样本数据前假设h拥有的初始概率，也就称为h的先验概率，它反映了我们所拥有的关于h是一个正确假设的机会的背景知识。当然如果没有这个先验知识的话，在实际处理中，我们可以简单地将每一种假设都赋给一个相同的概率。类似，P(D)代表将要观察的训练样本数据D的先验概率（也就是说，在没有确定某一个假设成立时D的概率）。然后是P(D/h)，它表示假设h成立时观察到数据D的概率。在机器学习中，我们感兴趣的是P(h/D),也就是给定了一个训练样本数据D,判断假设h成立的概率，这也称之为后验概率，它反映了在看到训练样本数据D后假设h成立的置信度。（注：后验概率p(h/D)反映了训练数据D的影响，而先验概率p(h)是独立于D的）。
 
 
 


P(h|D) = P(D|h)P(h)/p(D),从贝叶斯公式可以看出，后验概率p(h/D)取决于P(D|h)P(h)这个乘积，呵呵，这就是贝叶斯分类算法的核心思想。我们要做的就是要考虑候选假设集合H，并在其中寻找当给定训练数据D时可能性最大的假设h（h属于H）。
 
      简单点说，就是给定了一个训练样本数据（样本数据已经人工分类好了），我们应该如何从这个样本数据集去学习，从而当我们碰到新的数据时，可以将新数据分类到某一个类别中去。那可以看到，上面的贝叶斯理论和这个任务是吻合的。
 
朴素贝叶斯分类
 


  

也许你觉得这理论还不是很懂，那我再举个简单的例子，让大家对这个算法的原理有个快速的认识。（注：这个示例摘抄自《机器学习》这本书的第三章的表3-2.）
 
假设给定了如下训练样本数据，我们学习的目标是根据给定的天气状况判断你对PlayTennis这个请求的回答是Yes还是No。
 





Day
 

Outlook
 

Temperature
 

Humidity
 

Wind
 

PlayTennis
 



D1
 

Sunny
 

Hot
 

High
 

Weak
 

No
 



D2
 

Sunny
 

Hot
 

High
 

Strong
 

No
 



D3
 

Overcast
 

Hot
 

High
 

Weak
 

Yes
 



D4
 

Rain
 

Mild
 

High
 

Weak
 

Yes
 



D5
 

Rain
 

Cool
 

Normal
 

Weak
 

Yes
 



D6
 

Rain
 

Cool
 

Normal
 

Strong
 

No
 



D7
 

Overcast
 

Cool
 

Normal
 

Strong
 

Yes
 



D8
 

Sunny
 

Mild
 

High
 

Weak
 

No
 



D9
 

Sunny
 

Cool
 

Normal
 

Weak
 

Yes
 



D10
 

Rain
 

Mild
 

Normal
 

Weak
 

Yes
 



D11
 

Sunny
 

Mild
 

Normal
 

Strong
 

Yes
 



D12
 

Overcast
 

Mild
 

High
 

Strong
 

Yes
 



D13
 

Overcast
 

Hot
 

Normal
 

Weak
 

Yes
 



D14
 

Rain
 

Mild
 

High
 

Strong
 

No
 


 可以看到这里样本数据集提供了14个训练样本，我们将使用此表的数据，并结合朴素贝叶斯分类器来分类下面的新实例：
 
(Outlook = sunny,Temprature = cool,Humidity = high,Wind = strong)
 
我们的任务就是对此新实例预测目标概念PlayTennis的目标值(yes或no).
 
由上面的公式可以得到：
 


可以得到：
 
      P(PlayTennis =yes) = 9/14 = 0.64,P(PlayTennis=no)=5/14 = 0.36
 
      P(Wind=Stong| PlayTennis =yes)=3/9=0.33,p(Wind=Stong| PlayTennis =no)=3/5 = 0.6
 
其他数据类似可得，代入后得到：
 
P(yes)P(Sunny|yes)P(Cool|yes)P(high|yes)P(Strong|yes) = 0.0053
 
P(no)P(Sunny|no)P(Cool|no)P(high|no)P(Strong|no)=0.0206
 
因此应该分类到no这一类中。
 
 
 
贝叶斯文本分类算法
 
      好了，现在开始进入本文的主旨部分：如何将贝叶斯分类器应用到中文文本的分类上来？
 
根据联合概率公式（全概率公式）
 
 
 


M——训练文本集合中经过踢出无用词去除文本预处理之后关键字的数量。


作者：洞庭散人
 
出处：http://phinecos.cnblogs.com/　　　　
 

本博客遵从Creative Commons Attribution 3.0 License，若用于非商业目的，您可以自由转载，但请保留原作者信息和文章链接URL。
 







分类: Others, Search Engine
 

绿色通道： 好文要顶 关注我 收藏该文与我联系 




Phinecos(洞庭散人)
 关注 - 0
 粉丝 - 405 



+加关注 


3

0


 (请您对文章做出评价) 


« 博主上一篇：Winpcap网络开发库入门
» 博主下一篇：基于朴素贝叶斯分类器的文本分类算法（下）
« 首页上一篇：JQuery AJAX 目录浏览与编辑的实现
» 首页下一篇：升级Sql Server 2000到Sql Server 2005中要注意的问题

 
posted on 2008-10-21 14:49 Phinecos(洞庭散人) 阅读(19019) 评论(29) 编辑 收藏


 

评论

#1楼[楼主] 2008-10-21 15:06Phinecos(洞庭散人)  



自己坐个沙发，欢迎指教，下部还在写作中，很快放上来

支持(0)反对(0)
   

#2楼2008-10-21 15:27悟不透  



看了个开头，费劲回想这些怪异的公式符号来看懂文章已经导致了我大脑几乎抽筋。 
岁月不饶人啊。回复一个再慢慢看吧。

支持(0)反对(0)
   

#3楼2008-10-21 15:32fwaef[未注册用户]



ls的，基于贝叶斯的机器学习是最简单的了， 还没上神经网络呢，呵呵
  

#4楼[楼主] 2008-10-21 15:33Phinecos(洞庭散人)  



@fwaef
呵呵，是的，这确实是很简单的分类模型了，

支持(0)反对(0)
   

#5楼[楼主] 2008-10-21 15:33Phinecos(洞庭散人)  



@悟不透
理论无聊，待会直接看代码吧，呵呵

支持(0)反对(0)
   

#6楼2008-10-21 15:42悟不透  



@fwaef 
谁让咱上学的时候就忙着想怎么不上课了呢。 
现在脑袋抽筋哎。 
自己经受到教训以后才知道痛心:( 


支持(0)反对(0)
   

#7楼2008-10-21 16:15球球  



傻傻的看了5分钟，最后明白过来了，这不逆概率的公式嘛，只不过可能有个别名叫贝叶斯公式........真的差点“被噎死”了。

支持(0)反对(0)
   

#8楼[楼主] 2008-10-21 16:17Phinecos(洞庭散人)  



@球球
见《概率论和数理统计》课本第一章

支持(0)反对(0)
   

#9楼2008-10-21 16:18球球  



@Phinecos(洞庭散人) 
毕业8年了，上哪还记得别名，能记得逆概已经不错了.....

支持(0)反对(0)
   

#10楼2008-10-21 16:53so funny[未注册用户]



定义 设S为试验E的样本空间，B1, B2, …Bn为E的一组事件，若BiBj≠Ф, i≠j, i, j=1, 2, …,n; B1∪B2∪…∪Bn=S则称B1, B2, …, Bn为样本空间的一个划分。
 
呃，一个划分好像不该有交集吧……集合论里说的……
  

#11楼[楼主] 2008-10-21 17:03Phinecos(洞庭散人)  



@so funny
哦，是错了，修正

支持(0)反对(0)
   

#12楼2008-10-22 11:46巫云  



真强，把书都搬上了，兄弟辛苦了！

支持(0)反对(0)
   

#13楼[楼主] 2008-10-22 20:42Phinecos(洞庭散人)  



@巫云
懒得写了，直接copy

支持(0)反对(0)
   

#14楼2008-10-23 16:04自由[未注册用户]



年纪大了，慢慢看
  

#15楼2008-11-20 21:23杨海[未注册用户]



我只能说老兄你很牛 
谢谢你 
我在做这个的毕业设计！ 
一直找不到想要的资料！
  

#16楼2009-04-28 09:23王迎春[未注册用户]



 
@Phinecos(洞庭散人) 
你好，我是本科学数学的，现在在上计算机的研究生，看了你的帖子，我觉得非常棒，最近我准备做一场关于贝叶斯的报告，你所说的这些内容我都懂，但是我不知道该怎么组织，谢谢你组织出这么优秀的帖子。思路太清晰了。我还要准备贝叶斯信念网络和层次贝叶斯的分类过程，不知道你对这方面有研究么？？我找不到资料看。希望你能提供帮助给我。我的邮箱：yingchun48@sina.com
 希望能和大家共同交流。
  

#17楼2009-06-15 03:15gglong[未注册用户]



Bayes定理有没有写错？按书上说的 
P(B)=∑P(B｜Aj)P(Aj) 
P(Ai∣B)=P(B｜Ai)P(Ai)/P(B) 
得到 
P(Ai∣B)＝P(B｜Ai)P(Ai)/∑P(B｜Aj)P(Aj) 
就算换一下AB得到 
P(Bi|A)=P(A | Bi)P(Bi)/∑P(A | Bj)P(Bj) 

怎么会是这个呢 
P(Bi∣A)=P(A∣Bi)P(Bi)/∑P(B｜Aj)P(Aj)=P(B｜Ai)P(Ai)/P(B) 
请教
  

#18楼2009-06-15 03:34gglong[未注册用户]



假设现在有一个新病人，化验测试返回阳性，是否将病人断定为有癌症呢？我们可以来计算极大后验假设： 
P(阳性|cancer)p(cancer)=0.98*0.008 = 0.0078 
P(阳性|无cancer)*p(无cancer)=0.03*0.992 = 0.0298 
因此，应该判断为无癌症。 


这个结论有点不可思议。这里应该计算的是p(cancer|阳性)＝P(cancer)P(+|c)/P(+)=0.2085,由此可以得出得癌症的可能性并不大。 
什么是极大后验假设，怎么就由检出是阳性和极大后验假设就得出无癌症的呢？ 
我新学，如果方便请您解释一下
  

#19楼2009-07-25 14:35easier[未注册用户]



癌症病人的例子写错了吧。求的应该是P(cancer|阳性)
   

#20楼2009-08-31 11:12小阿[未注册用户]



谢谢楼上两位，我说我看了半天没看明白。。。
  

#21楼2009-09-18 10:38Google优化  



写得不错！跟书上有什么区别吗？google优化

支持(0)反对(0)
   

#22楼2009-11-23 09:00encoreway[未注册用户]



@easier
没有错的。。
  

#23楼2010-01-01 20:36weicheng  



M——训练文本集合中经过踢出无用词去除文本预处理之后关键字的数量。
但是楼主在程序实现中取值0，是不是错了啊？M是否可以详细解释一下。

支持(0)反对(0)
   

#24楼2010-11-10 19:41Leo Zhang  



“贝叶斯文本分类算法”这一节的公式3有问题吧，分母因该是：p(x1,x2,x3......xn)

支持(0)反对(0)
   

#25楼2010-11-10 19:43Leo Zhang  



还有公式8应该是进行了Laplace校准了吧

支持(0)反对(0)
   

#26楼2010-12-06 14:53专心做一件事  



贝叶斯公式写错了吧。。。。。。。。。。。

支持(0)反对(0)
   

#27楼2011-07-19 16:52过路狼  



“贝叶斯文本分类算法”这一节的公式3有问题吧，分母因该是：p(x1,x2,x3......xn) 
是不是搞错了？

支持(0)反对(0)
   

#28楼2011-07-19 16:53过路狼  



博主出来澄清下啊……

支持(0)反对(0)
   

#29楼2011-08-01 08:54what to say  



Bayes定理等号左边写错了,应该是P(Ai|B)而不是P(Bi∣A)

支持(0)反对(0)
   

 


刷新评论刷新页面返回顶部
 

注册用户登录后才能发表评论，请 登录 或 注册，访问网站首页。

 
博客园首页博问新闻闪存程序员招聘知识库
 





最新IT新闻:
 · 剥开周鸿祎 天天向上背后的故事
 · 马云知天命：固执坚持不动摇
 · 若不是那五次拒绝！惠普或成今日之苹果
 · 新疆阿勒泰发现世界第四大铁陨石 引发官争民抢
 · 伊朗的猴子宇航员真的安全返回了？
» 更多新闻...

最新知识库文章:

 · HTML5之美
 · 每天工作的第一个小时，做什么？
 · Hadoop 分布式文件系统：架构和设计
 · 异常的代价
 · 从P1到P7——第八年

» 更多知识库文章... 




Powered by: 
博客园
Copyright © Phinecos(洞庭散人) 




导航
 博客园
首页
新随笔
联系
订阅
管理
 
统计
随笔 - 607 
文章 - 2 
评论 - 1458 
引用 - 117 

公告

 


联系方式 
新浪微博：http://weibo.com/phinecos 
Email: phinecos@gmail.com 

昵称：Phinecos(洞庭散人)
园龄：6年8个月
粉丝：405
关注：0
+加关注

 

搜索

 
 
我的标签
 

随笔分类(744)
 .Net(33) 
ACM(69) 
Android(10) 
Assembly(4) 
C/C++/VC++(231) 
COM/ATL/ActiveX(16) 
Flex(10) 
Java(123) 
linux(30) 
Mozilla扩展(16) 
Others(30) 
PHP(12) 
python(14) 
Search Engine(9) 
程序人生(32) 
计算机图形学(69) 
考研心得(36) 

随笔档案(607)
2013年1月 (1) 
2012年12月 (2) 
2012年5月 (1) 
2012年3月 (3) 
2012年2月 (4) 
2012年1月 (2) 
2011年12月 (1) 
2011年11月 (2) 
2011年10月 (2) 
2011年9月 (1) 
2011年8月 (1) 
2011年6月 (1) 
2011年4月 (5) 
2011年2月 (1) 
2011年1月 (1) 
2010年12月 (1) 
2010年11月 (1) 
2010年10月 (6) 
2010年9月 (6) 
2010年8月 (1) 
2010年7月 (4) 
2010年6月 (1) 
2010年5月 (10) 
2010年4月 (9) 
2010年3月 (3) 
2009年11月 (1) 
2009年10月 (5) 
2009年9月 (18) 
2009年8月 (9) 
2009年6月 (7) 
2009年5月 (16) 
2009年3月 (9) 
2009年2月 (5) 
2009年1月 (3) 
2008年12月 (17) 
2008年11月 (50) 
2008年10月 (23) 
2008年9月 (15) 
2008年8月 (13) 
2008年7月 (32) 
2008年6月 (26) 
2008年5月 (17) 
2008年4月 (14) 
2008年3月 (11) 
2008年2月 (2) 
2008年1月 (13) 
2007年12月 (14) 
2007年11月 (24) 
2007年10月 (12) 
2007年9月 (11) 
2007年8月 (24) 
2007年7月 (17) 
2007年6月 (6) 
2007年5月 (6) 
2007年4月 (17) 
2007年3月 (11) 
2006年12月 (3) 
2006年11月 (3) 
2006年10月 (17) 
2006年9月 (11) 
2006年8月 (22) 
2006年7月 (3) 
2006年6月 (26) 
2006年5月 (4) 

常去的站点
cnBeta 
codeproject 
developerWorks 
sourceforge 

我的好友
vandalor 
涛哥 

我的站点
我在CSDN的blog 

积分与排名
 积分 - 982466 
排名 - 37 

最新评论
 

1. Re:ZOJ1009 Enigma
j-(curLine[j]-'A');  可以给解释下这个的意思吗？--简单地快乐
2. Re:《基于MFC的OpenGL编程》Part 4 Drawing Simple 3D objects 
楼主请问一下为什么我的画面知道能在-1<z<1之间，如果超出了这个范围就不可见了？？--梦想狂人
3. Re:Base64编解码(C++版)
在网上看了这么多东西，真正能用的也就这一篇，谢谢！--宝马良驹
4. Re:TinyXML：一个优秀的C++ XML解析器
新手路过，谢谢楼主分析~~~--柚子5
5. Re:TinyXML：一个优秀的C++ XML解析器
瑕疵是不是tixmldocument对象直接定义使用而不是new出来，这是作为根节点而不是一般的节点，new出来后没有父节点协助析构是不是应该delete。--flytosunset
 
阅读排行榜
 

1. TinyXML：一个优秀的C++ XML解析器(57546)
2. 基于MFC的OpenGL绘图(24451)
3. Android实例剖析笔记（一）(23346)
4. COM组件开发实践（一）(22609)
5. 技术宅---我的网上抢火车票攻略(19179)
 
评论排行榜
 

1. 基于朴素贝叶斯分类器的文本分类算法（下）(109)
2. 基于MFC的OpenGL绘图(77)
3. 技术宅---我的网上抢火车票攻略(52)
4. 基于JMF RTP的音视频传输(46)
5. IIS默认网站（停止）错误ox8ffe2740解决办法(44)
 
推荐排行榜
 

1. TinyXML：一个优秀的C++ XML解析器(9)
2. 技术宅---我的网上抢火车票攻略（终极秒杀版）(8)
3. 技术宅---我的网上抢火车票攻略(6)
4. Android实例剖析笔记（八）(5)
5. Android实例剖析笔记（二）(5)
 
