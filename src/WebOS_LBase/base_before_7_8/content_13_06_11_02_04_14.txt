ThNdl dvics, pxls, prfrmnc, usr xprinc, 44.4% optimizd.
All Â©2012-2013 Andrew Baldwin (Twitter,Google+), nokian with own opinions. 
Full content RSS
Filming shaders
Last updated: Fri, 05 Apr 2013 09:00:00 GMT
TweetGoogle+
Some of the most interesting shaders are self contained 4 dimensional worlds defined entirely by maths. With a powerful GPU rendering them in real time at 60 frames per second, they can look utterly stunning.

But what to do in case you don't have the most powerful GPU to enjoy these?

Well, if you are willing to give up on interaction, another option is to render the shader frames to a file and create a video from them.

Real time shaders often need to cut some corners to achieve good frame rates, but when rendering to file you can use as much processing time as you have patience for to raise the quality of individual frames.

One obvious thing to do is to render very large frames and scale them down with a filter to the desired final resolution. This can smooth away any jagged edges in the scene while still preserving sharp details.

The same trick can be played in the time dimension - render multiple frames with a slightly different timestamp, and combine them together. The result is a beautiful motion blur which makes the resulting video appear smoother.

Hollywood has used this trick for years. The standard frame rate for films is a paltry 24 frames per second. But film cameras traditionally exposed the film for half of that time using a "180-degree shutter" (see this great animation to understand how it worked). The result was blurring of moving objects, and the film appearing smoother despite the low frame rate.

Combining these ideas, for each pixel of our film, let's render 16 times at slightly different positions and slightly different times, spread over half a frame to simulate 180-degree shutter.

Here is the result. First, rendered once per pixel:



Now 16x oversampled and motion blurred:



Choosing a good shader and putting it all together into a film rendered at a YouTube-friendly 30FPS, gives you something like this:

"Shader Worlds: Elevated"

That was Inigo Quilez' amazing "Elevated" shader, derived from his famous 2009 4k demo of the same name.

I rendered it with normal OpenGL rather than WebGL, using a small python program which may in time develop into something more fully featured: shaderfilm.py

Crunching Rays in your Browser
Last updated: Wed, 27 Mar 2013 09:00:00 GMT
TweetGoogle+
 

If you've been following me at all on twitter, you probably noticed I was spending some time recently playing with the great new shadertoy.com

The new shadertoy (not to be confused with the original one) is the latest and greatest WebGL-based online shader editor.

It takes the publishing features of the glsl sandbox, adds draft shader saving, updating of published shaders, and user profiles+commenting to create a more visible shader-developing community. Great work from all involved.

The first few articles on ThNdl were rooted firmly in basic 2D shader techniques. However, on shadertoy I've mainly been exploring 3D effects. A large number of the other shaders there are also 3D-based.

One reason for this is that WebGL lets you cut through the layers of abstraction that normally sit between you and the hardware when you are working in a browser. Shaders are compiled in your browser and run on the metal of your GPU. 3D scenes and effects can require and make more use of that raw processing power than 2D scenes.

Modern desktop GPUs can have hundreds or even thousands of seperate processing units. The shader you write runs on all of them at the same time, each instance rendering a different pixel. The latest cards can have a peak theoretical performance of 5 teraflops - that means 5,000,000,000,000 (5 trillion) floating point operations every second.

(Wow!)

So, what on earth can you sensibly do with a trillion (from a mid-range GPU) calculations per second? Well, not that much, it turns out.

Firstly, you want to animate something, ideally at 60 frames per second for a smooth realistic experience. So now you have only 16 billion calculations - per frame that is.

Of course you want to fill a screen, for example FullHD's 1920x1080 pixels. So now our staggering raw teraflops performance has been cut down to a less-than-impressive 8000 instructions per pixel per frame.

If you are rendering a traditional 3D scene modelled with lots of vertices and textures, this is more than enough for very complex scenes.

But many of the interesting shader experiments are based on rendering a scene described mathematically in code using techniques like ray tracing shapes, ray marching distance fields, or path tracing light emitters.

For these techniques, you often need or want to sample a 3D-function describing the world hundreds of times in order to find surfaces, check shadows, reflections and ambient occlusion. If you do that just 100 times, then that function describing the world must evaluate in a mere 80 instructions, otherwise you've blown your total instruction budget and your frame rate will start to drop.

That's how to use up a trillion instructions per second rendering (real time) 3D.

On mobile devices, 20-40 gigaflops is currently more common, going up to 100 gigaflops soon. But that's still just 2% of the performance of those desktop power guzzling monsters!

WebGL came first to the desktop browsers, and despite being based on the mobile version of OpenGL, it's been slow to come to mobile. On iOS it's limited to ads only (???) with no sign of that changing. On Chrome for Android it's now in the latest versions, but hiding behind a user flag - not switched on by default.

When it finally arrives on mobile for real, just take that same wonderful looking 3D shader running 60FPS on your desktop, and run in on your mobile WebGL browser. The results will not be pretty. In the best case you'll get one frame per second. If you're unlucky, the device might lock-up or reset.

This is a bit of a dilemma, that the same technology can span devices with such a wide range of performance. It's also unfortunate that the people most interested in WebGL at the moment are probably the ones with huge GPUs on their desktops. The result is that "mobile-compatible" WebGL content is practically non-existent at the moment because it requires deliberate effort to target a "low" performance level.

In the meantime, while I'm sitting at my desk I'll carry on burning up those teraflops trying to create mini 3D worlds with shadertoy. After all, you can't save them up for later.

Shader recap
Last updated: Thu, 29 Nov 2012 21:30:00 GMT
TweetGoogle+
So far I've posted 9 articles here covering different techniques for making OpenGL shaders.

For those of you that have been following and trying things out, I thought this might be a good place for a recap.

I introduced the different built-in functions of GLSL as we needed them to achieve different goals, but at some point you might have been wishing for a compact reference to all the available functions.

The one I use very often is the Khronos Group's 4 page PDF format "OpenGL ES 2.0 Quick Reference Card". Page 3 and 4 cover GLSL including all the built-in functions.

There is also an online OpenGL ES Shading Language reference page, which covers also the latest version 3.0 (most devices and WebGL still use the 1.0 version).

Now you have those under your belt, here is a list of all the functions we've used so far.

 
f = vec4(c.x, c.y, 0.0, 1.0);

Function: vec2(float,float), vec3(float,float,float), vec4(float,float,float,float)

Description: Vectors are groups of numbers.

These constructor functions are used to create them - pass single numbers or vectors as parameters.

For example, vec4(colour.rgb,1.) creates a new vec4 using the first 3 elements of the vector colour and the number 1.0 as the fourth element.

Single or multiple elements can be referred to using dot notation using rgba, xyzw or stuv.

For example v.xy would return a vec2 containing the x and y elements of vector v (which could be a vec2, vec3 or vec4).



 
float d = length(c.xy); 
f = vec4(d, d, d, 1.0);

Function: length(vec)

Description: Returns the length of the given vector.

The length of 2 dimensional vec2 v would be equivalent to sqrt(v.x*v.x+v.y*v.y).

Can be used to create a distance field for drawing circles and related shapes.

See The Pixel Swarm



 
float d = step(0.1, 1.0-length(c.xy)); 
f = vec4(0.0, 1.0, 0.0, d);

Function: step(threshold,value)

Description: Returns 0.0 if value is less than threshold, otherwise returns 1.0.

Used to break a continuous field into two distinct regions.

For example, it can be applied to a distance field calculated using length to create a circle.

See The Pixel Swarm



 
float d = smoothstep(0.08,0.1,1.-length(c.xy)); 
f = vec4(0.0, 1.0, 0.0, d)

Function: smoothstep(lower,upper,value)

Description: Returns 0.0 if value is less than lower, 1.0 if value is above upper, and smoothly interpolates the range between.

Interpolation is not straight/linear. It uses a function like f = f * f * (3.0 - 2.0 * f)

Used to break a continuous field into two distinct regions with a smooth transition.

For example, it can be applied to a distance field calculated using length to create a circle with smooth or blurred edges.

The greater the gap between lower and upper, the wider the transition region.

See The Pixel Swarm



 
vec2 r=abs(c.xy); 
f=vec4(r,0.,1.)

Function: abs(value)

Description: Returns the value unchanged if value is greater than zero, or -value if value is less than zero.

The effect is to make continuous values/fields reflect around zero.

Can be used in combination with mod or fract to help make continuous repeating functions.

See Square shaped shaders



 
vec2 r=abs(c.xy);
float s=max(r.x,r.y);
f=vec4(vec3(s),1.)

Function: max(first,second)

Description: Returns first if first is larger than second, else returns second.

Useful for creating squares (or cubes in 3D).

See Square shaped shaders



 
int N=7; 
float a=atan(c.x,c.y)+.2; 
float b=6.28319/float(N); 
f=vec4(vec3( smoothstep(.5,.51, cos(floor(.5+a/b)*b-a)*length(c.xy))),1.);

Function: atan(x,y)

Description: Returns the angle, in radians, between +pi and -pi, formed by the right-angled triangle with width x and height y.

Combined with length, it converts cartesian coordinates to polar coordinates.

Can be used for partioning space into regions based on their angle around a centre point.

See Square shaped shader



 
vec2 r=(456.789* sin(789.123*c.xy)); 
f=vec4(fract( r.x*r.y*(1.+c.x)))

Function: sin(value),cos(value),tan(value)

Description: sin and cos are continuous smooth functions in the range -1 to +1 which repeat over a period of 2*Pi. tan (equal to sin divided by cos) also repeats but is not continuous - or limited in range.

These functions are very useful for creating smooth variation, or with high frequencies, pseudo-random aliasing noise.

Depending on your platform, they can be quite expensive to calculate.

See Noise from numbers



 
vec2 s=floor(3.*(c.xy*.5+.5))/2.; 
f=vec4(s.x,s.y,0.,1.)

Function: floor(value)

Description: Returns the integer part of value, discarding the fractional part.

Used for partitioning continuous functions into regions sharing a common constant value which can be used to give all pixels in that region similar behaviour.

Combined with fract to split a value into two parts, it can be used for interpolating between regions.

See Continuously Discrete



 
vec2 m=mod(c.xy+c.x+c.y,2.)-1.;
f=vec4(length(m))

Function: mod(value, period)

Description: Returns the remainder of value divided by period.

Mod can be used to create a repeating function from a continuous function.

fract can be a cheaper alternative.

See The Art of Repepetition



 

vec3 sky=mix( vec3(.0,.0,.5), vec3(.0,.5,1.), c.y*.5+.5); 
float circle=smoothstep(.25, .3,1.-length(c.xy)); 
float shadow=smoothstep(.25, .3,1.-length(c.xy+vec2(.5,0.)));
f=vec4(mix( sky, vec3(.6), mix(circle,0.,shadow)),1.);
Function: mix(first,second,value)

Description: Returns first if value is 0 or less. Returns second if value is 1 or more. Between 0 and 1, returns a linear interpolated blend of first and second.

Mix is used for combining multiple overlapping regions. It can be used to select colours or shapes. Chaining mix results can create complex scenes without needing any conditional statements.

See Mixing it up a bit



 
float s=1.-length(c.xy*c.xy*c.xy*c.xy);
f=vec4(1.,.8,.6,smoothstep(0.,0.1,s));

Function: pow(value,raise)

Description: Returns value raised to the power raise. Raise can be integers - 2.0 would return value*value - or floats - 0.5 would return sqrt(value).

Useful for making squircles.

Can be an expensive function.

See Going round in squircles



 
vec2 d=abs(fract(c.xy)*2.-1.);
f=vec4(d.x*d.y)

Function: fract(value)

Description: Returns the fractional part of value.

Paired with floor, it breaks numbers into two parts - integer and fractional.

Paired with abs, it can create a repeating triangle wave which oscillates linearly between 0 and 1 - sometimes a cheaper alternative to sin.

Very useful for creating aliasing noise.

See More noise



 
vec2 d = clamp(abs(c.xy),0.2,0.8);
f = vec4(d.x*d.y);

Function: clamp(value,min,max)

Description: If value is less than min, returns min. If value is more than max, returns max. Otherwise returns value. Effects is to limit value to the range min-max.

Sometimes the output of one function can exceed the useful input range of another. clamp can constrain the range in this case.

One example is when doing calculations with colours. Display will naturally clamp to the range 0-1, but intermediate calculations using higher or lower values can lead to incorrect results which explicit clamping can correct.

More noise
Last updated: Fri, 23 Nov 2012 09:42:00 GMT
TweetGoogle+
Now we have a function which can give us a random-ish looking number between 0-1 for every pixel we render with a shader.

By using floor as we did earlier with continuous numbers, we can now have a random number for each different region of the image.

 
float rand(vec2 p) { 
ââp+=.2127+p.x*.3713*p.y; 
ââvec2 r=4.789*sin(489.123*(p)); 
ââreturn fract(r.x*r.y); 
} 
f=vec4(rand(floor(vec2(4.,2.)*c.xy)))

We've put our random number code into a reusable function called rand(), with a couple of improvements compared to the original version.

First we skew the coordinates to avoid obvious symmetry - that also helps give us less aliasing patterns. That then allows the multiplier to be reduced to give us more precision when zooming out.

By scaling our coordinates and applying floor before calling the rand function, we can control how many regions we want.

Putting together many techniques from earlier articles gives us, as an example, 32 random hued squircles.

 
float rand(vec2 p) { 
ââp+=.2127+p.x*.3713*p.y; 
ââvec2 r=4.789*sin(489.123*(p)); 
ââreturn fract(r.x*r.y); 
} 
vec3 h2r(vec3 hsv) { 
ââvec3 t=clamp(abs(mod(hsv.r*6.+ vec3(0.,2.,4.),6.)-3.)-1.,0.,1.); 
ââreturn hsv.b*hsv.g*t+hsv.b-hsv.b*hsv.r; 
} 
vec2 p=c.xy*vec2(4.,2.); 
float s=smoothstep(.1,.2,1.-length(pow(2.*fract(p)-1.,vec2(2.)))); 
vec3 t=h2r(vec3(rand(floor(p)),.75,.6)); 
f=vec4(t,s)

Ok, so that's one basic use for randomness to raise the interest level of shaders.

The next thing we need to make is, a little perversely, smoothed noise.

You see, the problem with the noise we've made so far is that it's "white noise" with about the same amount of power at all frequencies.

By smoothing - filtering - it, we can create a different spectrum of noise. The simplest way to do this is to use the floor to calculate the 4 random values around each point, and then blend those values based on how far the point is from each one.

Something like this:

 
float rand(vec2 p) { 
ââp+=.2127+p.x*.3713*p.y; 
ââvec2 r=4.789*sin(489.123*(p)); 
ââreturn fract(r.x*r.y); 
} 
float sn(vec2 p){ 
ââvec2 i=floor(p-.5); 
ââvec2 f=fract(p-.5); 
ââfloat rt=mix(rand(i),rand(i+vec2(1.,0.)),f.x); 
ââfloat rb=mix(rand(i+vec2(0.,1.)),rand(i+vec2(1.,1.)),f.x); 
ââreturn mix(rt,rb,f.y); 
} 
f=vec4(vec3(sn(vec2(4.,2.)*c.xy)),1.)">

That was blending with 3 normal mix statements - bilinear smoothing. It would look much better with one line added to modify the fract and make a non-linear blend.

The equivalent of a smoothstep would be f=f*f*(3.0-2.0*f). But an even better alternative (which is smooth also at higher derivatives) comes via Inigo Quilez: f = f*f*f*(f*(f*6.0-15.0)+10.0)

 
float rand(vec2 p) { 
ââp+=.2127+p.x*.3713*p.y; 
ââvec2 r=4.789*sin(489.123*(p)); 
ââreturn fract(r.x*r.y); 
} 
float sn(vec2 p){ 
ââvec2 i=floor(p-.5); 
ââvec2 f=fract(p-.5); 
ââf = f*f*f*(f*(f*6.0-15.0)+10.0); 
ââfloat rt=mix(rand(i),rand(i+vec2(1.,0.)),f.x); 
ââfloat rb=mix(rand(i+vec2(0.,1.)),rand(i+vec2(1.,1.)),f.x); 
ââreturn mix(rt,rb,f.y); 
} 
f=vec4(vec3(sn(vec2(4.,2.)*c.xy)),1.)">

Finally, we can create a controlled spectrum of noise by using this function to generate smooth noise at different frequency octaves.

For example, we can double the frequency each time, and then add them all together with different strengths. The most common approach is to halve the strength of each octave higher. Like this:

 
float rand(vec2 p) { 
ââp+=.2127+p.x*.3713*p.y; 
ââvec2 r=4.789*sin(489.123*(p)); 
ââreturn fract(r.x*r.y); 
} 
float sn(vec2 p){ 
ââvec2 i=floor(p-.5); 
ââvec2 f=fract(p-.5); 
ââf = f*f*f*(f*(f*6.0-15.0)+10.0); 
ââfloat rt=mix(rand(i),rand(i+vec2(1.,0.)),f.x); 
ââfloat rb=mix(rand(i+vec2(0.,1.)),rand(i+vec2(1.,1.)),f.x); 
ââreturn mix(rt,rb,f.y); 
} 
vec2 p=c.xy*vec2(4.,2.); 
f=vec4(vec3( 
.5*sn(p) 
+.25*sn(2.*p) 
+.125*sn(4.*p) 
+.0625*sn(8.*p) 
+.03125*sn(16.*p)+ 
.015*sn(32.*p)),1.)

This kind of cloudy pattern is usually called Fractional Brownian Motion (FBM). As you can see, it's very expensive to calculate - in our case needing no less than 48 sin() calls for each pixel - ouch!

But it makes an incredibly useful and reusable building block for modulating and colouring other simpler functions to give shader images a more natural or organic feeling.

Here's one example to end with, an instant world map.

 

Noise from numbers
Last updated: Thu, 22 Nov 2012 00:00:00 GMT
TweetGoogle+
 

Sometimes, a little noise is all you need to go from synthetic to organic.

The world is unpredictable, and that makes it interesting.

The same rules applies to shaders - unpredictable output makes for more interesting and natural-looking results. So how do you make a shader output unpredictable results?

An easy way is to feed it random numbers for every pixel in an image texture and use those in each calculation. But let's rule out that approach for now.

(It's anyway not always appropriate for complex shaders because a shader may end up doing many texture lookups from different parts of memory, and that can become a performance bottleneck).

Let's say you want the shader to generate unpredicatble output given smooth simple predicable input (and no textures).

To start with, there is no random number generator in GLSL. So you can't just call some built-in function like random() like you might on a CPU.

You need to provide your own function for this, one that takes any number as a parameter, mixes it up a bit, and returns another number in a known range (like between 0 and 1).

The output numbers for two close inputs should be very different so that when we show the output, we cannot see obvious patterns.

Unfortunately normal GLSL is also missing bit-level operations like shift and exclusive-or that would otherwise be very useful for making this kind of function.

Instead we need to make use of something that in other situations is annoying, but in this case will be useful - aliasing.

Aliasing is what happens when you sample a function at lower than double the frequency of the highest frequency component it contains. If you do that without filtering out the high frequencies first, they "reflect" and interfere with the rest of the signal.

When it happens with sound the result is often a ringing echo or sharpness. With images (say when you draw an image at half size using a "nearest neighbour" algorithm) it shows as sharp and pixellated edges or shimmering animation.

But today, for this problem, aliasing is going to be our friend by helping us to make some noise.

We need to start with a repeating function, like sin.

 float r=.5+.5*sin(10.*c.x); f=vec4(r)

To get aliasing, and potentially noise, we need to increase the frequency.

 float r=.5+.5*sin(789.123*c.x); f=vec4(r)

Hmm.. that didn't work. It is aliasing, but it's still giving us a regular pattern.

What we need is to combine it with another function - fract, which returns just the fractional part of a number, and as a side benefit will automatically give us a result in the range 0-1. This is essentially another repeating function which will show aliasing.

 float r=fract(sin(789.123*c.x)); f=vec4(r)

Some signs of noise, but we're not there yet. We need to increase again the frequency of output of the fract, by increasing the amplitude of the sin.

 float r=fract(456.789*sin(789.123*c.x)); f=vec4(r)

Finally, some noise! Though it's only in 1-dimension. We need to try and turn this into 2D TV-static somehow.

 vec2 r=fract(456.789*sin(789.123*c.xy)); f=vec4(r.x*r.y)

Now we calculated the initial noise values based on both x and y coordinates. Then we combined the results by multiplying. But that wasn't very effective as the result has a clear pattern. Perhaps our fract is in the wrong place now?

 vec2 r=(456.789*sin(789.123*c.xy)); f=vec4(fract(r.x*r.y))

That's more like it. Now we have mostly the noise we were after, though there are still some hiddern patterns which brains do an amazing job at spotting. One cheap extra term should fix that.

 vec2 r=(456.789*sin(789.123*c.xy)); f=vec4(fract(r.x*r.y*(1.+c.x)))

There, the patterns seem to have gone now. The final instruction tally per pixel with this approach is 2 sin, 1 fract, 6 multiplies and 1 add. That's not horrifyingly expensive, but sin can be a little painful.

Before this noise becomes useful for making images, we'll need to do some more work with it. But that's going to be the subject of another article.

Travelling in parallax
Last updated: Thu, 15 Nov 2012 00:00:00 GMT
TweetGoogle+
This won't be a long one. It's really just an excuse to show you some new background scenery.

(If you're reading the RSS, why not stop by the web site now to see what I'm talking about?)

Unfortunately it will probably be quite boring (disabled) on Firefox due to trouble with image upscaling, but if you are using Chrome or Safari (webkit browser), especially an iPad, it might be quite pretty.

So have a look round, read some of the older articles with a new perspective. Or go and explore the far right side of the blog.

In case you are curious, the parallax effect is mostly done with CSS 3d transforms and transitions applying perspective to a stack of semi-transparent PNG files, generated server-side using shaders.

The CSS3 feature -transform-style: preserve-3d; is the main trick to make the browser give depth to a scene as it pans around. With that applied to a parent item, and each image in the stack given a different Z depth from the screen using a translate, then the browser moves them at different rates.

Unfortunately it can't be applied to any scrolling item because on webkit, overflow:scroll is not compatible with the preserve-3d option. So the 3d scene is put into a parallel tree behind the articles and repositioned using javascript triggering a CSS transition. That's not ideal, especially on iOS where scroll events do not come in regularly after a flick. So the parallax can't track the panning rigidly.

Still, not a bad effect.

EDIT:22/11/2012. The parallax effect lasted a week before I changed to a new and slightly more conservative style for the blog.

Native means nothing
Last updated: Wed, 07 Nov 2012 11:30:00 GMT
TweetGoogle+
For a change, this article is not about shaders. But the illustrations are all shaders you can click on and edit.

A word that is often thrown around (very often without definition) in the mobile industry these days is native.

It's typically used in the context of application platforms, frameworks, and ecosystems, suggesting that the one with the native label is somehow closer to the hardware, which implies (depending on which camp the labeller is sitting in) either harder to use (bad) or giving you better performance (good).

A strict definition of native code would probably involve compilation before deployment into instructions executed directly by hardware.

My assertion is that even if something is strictly native, that does not automatically mean it is any better or worse than a competing non-native equivalent.

I see many different kinds and levels of virtual machines and abstractions as you roam up and down the software stack. Some might be labelled native, other not. But each of those abstractions need to be judged and compared on their actual merits and failings, not solely by a label.

(An aside: ecosystem is an interesting choice of metaphor for the mobile application platforms. They tend to be ecosystems that involve one giant and heavily armed animal trapping thousands of defenceless small animals, making them fight against each other for survival with two paws tied together, and then finally stealing a share of all the food they collect. If that was a natural ecosystem would the small animals quickly die, or else work out how to untie their paws and keep all their food?)

 

Like many people I know (and possibly some of you reading), I've spent more than a few years now assembling the operating systems for many kinds of different phones and devices - some you might have seen and used, and others that you never got the chance to try.

In that time I've been lucky enough to work at many levels of the giant software stacks in those devices, from kernels and device drivers sitting bare on the hardware, through systems and servers in the middle, up to UI frameworks and applications at the top that you - with your user hat on - see and interact with directly.

 

If you've been a user of devices like smartphones, but not actively involved with making the software that runs them, you might not fully appreciate that these are some of the most complex systems humans have created to date.

To start with, there are a staggering number of individual processing cores in a typical device like an iPhone. Normally we just focus attention on the big and fast CPU cores, of which the latest devices typically have 2 (dual) or 4 (quad).

But often many subsystems and peripherals integrated inside the chip at the heart of the device can have full independantly-executing processing cores of their own. Integrated graphics processing units (GPUs) have multiple shader units. Modems can have multiple ARM and DSP cores. WiFi & Bluetooth peripherals can have another ARM core dedicated for running those protocols. Multimedia subsystems can have DSPs or ARM cores for audio and video processing.

Since it's the responsibility of the operating system in the phone has to tie all those pieces together and make them work as one consistent whole, that naturally makes operating systems very complex, typically needing many millions of lines of source code.

 

Earlier, I've had the chance to spend some time working a little bit outside the box, with a performance profiling tool that gives you the complete picture of exactly what every process, thread and module of an operating system is doing at once, and how they are interacting with each other, at a resolution of billionths of a second - which is the timescale that current systems operate at. Exploring systems and use cases with that was a very educational experience.

Every piece of software is written to work in a particular kind of environment. Part of that environment comes from the language the software is written in. Another part comes from how that language is finally executed. Yet another part is from the runtime environment it executes within. And finally there are the APIs and interfaces offered to interact with the world outside the environment - files, network, displays, etc.

All of these things are abstractions - constructed on top of some other lower level environment, (ideally) with a goal of making it easier to achieve things than it would have been if done directly with that underlying environment.

At the top of the abstraction stack we have the high-level app frameworks, with environments like Java, Javascript, QML, or HTML5.

Underneath those come lower-level application environments based on C++ or ObjectiveC compiled to hardware-executed instructions, backed up with a large set of libraries and service APIs.

Below those, we typically stay with compiled C-languages, but strip away the dependencies on other services until the lowest level "user space" components depend only on something like a C library and some interfaces from the kernel.

At this point, we are still working in virtual machines - typically called threads or processes - that conveniently hide away messy details like which page in which RAM chip our memory blocks came from.

Jumping down from there we reach "kernel space". What that means in practice is that the CPU switches to a different (priviledged or supervisor) mode which gives the code running there much more control and visibility. The developers at this level still work with C or even hand-written assembler code, but they usually have a different and simpler set of abstractions to build on. Now we are underneath or outside those user-space virtual machines, able to see the "real" memory pages they are using, for instance.

It doesn't stop there. The kernel itself can be living inside a virtual machine like a hypervisor, which might route some hardware events or interrupts to be handled by a different parallel software stack unknown to the kernel.

Going down further we get to a high-level hardware view of the world, and enter a different universe of parallel blocks, clock domains, buses, caches, and other logical parts that come together to create the virtual environment executing the kernel or hypervisor.

Slightly surprisingly, hardware is often developed with software-like languages and abstractions. Due to the high financial and schedule cost of mistakes, hardware developers spend much more time simulating and testing their code before comitting it to silicon (it's always fun when hardware and software developers try to communicate and agree things - a bit like like venus and mars).

We could go further down - gates, electrons, quantum effects and subatomic particles, and finally out to the fundamental universe simulation we all think we're living in. (Woah, toooo deeeep. Emergency surface!)

 

Where was I? Oh yes. Levels of abstraction.

The higher you go, the more amplifying the abstractions tend to be. By this I mean that a small set of operations can cause very much action from the system underneath. Powerful, but also a little bit dangerous.

The lower you go, the more direct the abstractions tend to be. For instance, they are often concerned with controlling the state of actual pieces of hardware, or HW subsystems of the device you are running on.

Any abstraction can be designed well, or designed badly.

A good abstraction gives you expressive power to achieve a goal with less effort, in part because it handles many (lower level) problems for you. In the best case it can often do this with good or better performance than a naive hand-made solution working at a lower level.

On the other hand, a bad abstraction can hide things from you that could be easily achieved working at a lower level. Or even worse, it could totally fail to handle lower level problems for you while also leaving you unable to solve them either, due to hiding the lower levels from you.

Descriptions like native and non-native are not synonyms for good and bad abstractions.

I've used complex but poorly-performing libraries and frameworks based entirely on C++.

I've also used productive and high-performance libraries based on interpreted languages.

The performance of many software-based virtual machines has increased hugely in recent years. An "arms race" in Javascript just-in-time (JIT) compilation engines has made some cases rival traditional compilation. Similarly in the Java and Python (PyPy) worlds.

Flexible compiler frameworks like LLVM blur the lines further about exactly when and how the final hardware instructions are created and executed, for example allowing many specialised versions of a function optimised at run time for each different kind and value of data being processed - something that isn't feasible with ahead-of-time compilation.

Taking my own favourite area - shader programs. These are usually distributed and deployed as text-based source code in a hardware-neutral language (like GLSL). They are then compiled on demand on the device running the application into hardware instructions, which are then uploaded to a graphics processing unit and run in parallel on many tiny processors. This approach can be orders of magnitude faster that an equivalent C++ program run on a traditional CPU. Is that approach native?

The most important thing is to really study and benchmark an abstraction, to determine if, in actual practical use, it is well designed and bringing more value than costs.

Some of the questions to ask are:

Can problems be solved with less effort using this abstraction than the level below it?
Are the problems really solved, or do solutions using the abstraction actually tend to create new problems like reduced performance or increased memory consumption?
Does the abstraction prevent features of the level below from being used? If so which ones, and is their loss a good trade for the features the abstraction brings?
Does the abstraction offer an extension or binding route to the level below? If so, is using that still less work than entirely working at the level below? (not always the case!)
Keep your wits about you. Don't be swayed either way by something being labelled native or not. Look for the good abstractions that solve old problems for you, and help you solve new ones with less effort.

Challenge or fix or bypass the bad abstractions that claim to make 80% of the development or deployment easier, but in reality make the final 20% impossible.

And watch out for giant animals.

Continuously discrete
Last updated: Mon, 05 Nov 2012 22:00:00 GMT
TweetGoogle+ 
Smooth looking images have every pixel quite similar to the ones all around them, while still being different enough to form a picture that isn't just one solid colour.

When a shader program is used to make an image, it is typically fed with input values (say x and y coordinates) that are only slightly different for each pixel.

If those slight differences are preserved in the calculations the shader makes, the output image will be smooth.

Interesting images however need changes in them. Tools like step, mod and abs are good for creating those, but still they tend to generate quite predictable images, which are actually not so interesting.

What we could really make use of is a way to create discrete values from continuous values. That would let us make different parts of an image clearly different.

Say I want to make a very simple picture - four coloured squares, each one a different colour. Let's try with step first:

vec2 s=step(0.,c.xy); 
f=vec4(s.x,s.y,0.,1.) 
 

Applying the step to x and y makes 4 regions around the center (0,0) point. Negative values are zero, positive values are 1. Mapping x and y to red and green gives us four colours - 0+0=black,0+1=green,1+0=red,1+1=yellow.

Can the same approach now gives us 9 different coloured squares?

vec2 s=0.5*(step(-0.333,c.xy)+step(0.333,c.xy)); 
f=vec4(s.x,s.y,0.,1.) 
 

Well, yes. Adding the result of 2 steps at minus and plus 1/3rd and scaling the result to be between 0 and 1, now with intermediate values of 1/3 and 2/3, worked. But it's clear this approach won't scale up if we want 10 or 100 steps.

So, what would be another approach? If we would scale up our input range to the number of steps we want, in this case 0-3, and then simply throw away the fractional parts, so that 1.2 becomes 1, and 2.3 becomes 2, and finally scale the result back, that should give us the same result.

The GLSL function we need here is floor. Let's try it out:

vec2 s=floor(3.*(c.xy*.5+.5))/2.; 
f=vec4(s.x,s.y,0.,1.) 
 

The looks practically identical to the double step version, but now we only have one function. The coordinate calculation was a bit messy because we had to scale from -1-1 to 0-3, then we divided by 2 to get the result back in the range 0-2.

(You may have spotted that at the right edge the result is 3, not 2, which scales back down to 1.5 instead of 1. But it doesn't matter in this case because that still clamps to 1 when used as a colour value).

The big advantage of our new approach is that it scales to any number of steps. Here's a 5x10 grid which is almost the same as the 3x3. But this time, instead of a gradient, I'm going to use it to count to 50. In shades of grey.

vec2 s=floor(vec2(5.,10.)*(c.xy*.5+.5)); 
f=vec4(0.02*(s.x+5.*s.y)) 
 

What do you mean that wasn't as interesting as you expected? Ok, how about this.

Using our new-found ability to count in a shader, why don't we combine that with something we developed earlier. Here's 50 shapes. All grey, of course.

vec2 r=vec2(5.,10.)*(c.xy*.5+.5); 
vec2 i=floor(r); 
vec2 c=(fract(r)-.5)*vec2(4.,2.)*.6; 
float s=3.+i.x+5.*i.y; 
float a=atan(c.x,c.y); 
float b=6.28319/s; 
float w=floor(.5+a/b); 
float g=smoothstep(.55,.5,cos(w*b-a)*length(c.xy)); 
f=vec4(vec3(1.-g*((60.-s)/50.)),g); 
 

The polygon shader from earlier took a parameter for the number of sides. By feeding the counting value to the polygon shader, we get polygons with increasing numbers of sides depending on the count value in that region of the image. It also uses the count to choose the grey shade.

But I think I've had enough of grey shades for now, and I've also had enough of RGB.

A different model for mixing colours is HSV - Hue (colour), Saturation (whiteness) and Value (darkness). A nice feature of hue is that it repeats - a hue of 1 is the same as a hue of 0.

I'm going to finish now by using our count to pick a hue, the polygon side to pick a value, and the distance from the centre to pick a saturation.

That should make, well, something.

vec2 r=vec2(5.,10.)*(c.xy*.5+.5); 
vec2 i=floor(r); 
vec2 c=(fract(r)-.5)*vec2(4.,2.)*.6; 
float s=3.+i.x+5.*i.y; 
float a=atan(c.x,c.y); 
float b=6.28319/s; 
float w=floor(.5+a/b); 
float g=smoothstep(.55,.5,cos(w*b-a)*length(c.xy)); 
float v=1.-1.2*length(c.xy); 
float sa=1.-abs(w/s); 
float h=s/50.; 
vec3 rgb=v*sa*clamp(abs(mod(h*6.+vec3(0.,2.,4.),6.)-3.)-1.,0.,1.)+(v-v*sa); 
f=vec4(1.-g*rgb,g); 
 

Going round in squircles
Last updated: Mon, 29 Oct 2012 21:00:00 GMT
TweetGoogle+
If you were an N9 user, you might have noticed something about the shapes of many of the icons it used.

They weren't quite the usual squares, or even rectangles with rounded corners, but another shape called (I kid you not) a squircle, which if that wikipedia link is to be believed, is a kind of "superellipse".

Here's a picture of a squircle (in red) and a similar sized rounded rectangle (in green) so you can see the subtle differences: 
The corners are less curved than the rounded rectangle.

I described earlier how to make a rounded rectangle shader in the Square shaped shaders article.

To make a squircle, again we start by using length, though it won't be quite a distance field we make. Wikipedia says we need to have a power of 4. So lets try this one: 

f=vec4(1.-length(c.xy*c.xy*c.xy*c.xy)); 

It already looks quite squircley, doesn't it? That's before we've even applied a step function.

Here it is with the (smooth)step to make a solid shape with smooth edges: 

float s=1.-length(c.xy*c.xy*c.xy*c.xy);
f=vec4(smoothstep(0.,0.1,s)); 

I think it's quite nice - a bit more natural than plain rounded corners. It makes a nice mask or alpha channel for a PNG.

Here it is as the border for a beautifully calm and relaxing sky photo: 
Nah, that wasn't a photo after all, just another shader.

There is another way to write the squircle function which will lead us somewhere new. Instead of c.xy multiplied together 4 times, we can use the GLSL pow function, like this: 

f=1.-length(pow(c.xy,vec2(4.)));

We already know that a power of 1 will give us a circle. Well, a large power will give us something very close to a square. But what about other numbers?

Here's a power of 3: 


Slightly curvier corners. Now a power of 2: 


Even curvier. Next 1.5 - yes, powers don't have to be integers: 


Now 1.25: 


We'll skip 1 - the circle. How about 0.85: 


0.5 is surprising: 


A diamond!

And a last one to end with, 0.25: 


Mixing it up a bit
Last updated: Mon, 29 Oct 2012 06:00:00 GMT
TweetGoogle+
With just a few shader functions like length, mod and max, I can make simple repeating shapes like squares, circles, and lines. With stretching and skewing of the coordinates, these can be moulded in all kinds of interesting ways.

But still, when I'm trying to create a more complex image, more techniques are needed.

One function that opens up a whole new world of possibilities is the GLSL blending function mix.

This takes 3 parameters - two values, which can be either scalar floats or vectors like colour values - and a blend value, which is a float between 0.0 and 1.0.

When the blend value is zero, the result is the first value, and when it is one, it is the second value. But in between, the result is a linear mix of both values.

This probably becomes clearer with an example: 

f=vec4(mix( vec3(.0,.0,.5), vec3(.0,.5,1.), c.y*.5+.5),1.) 

 
In this case we are mixing solid colours depending on the height in the image- dark blue at the top, and sky blue at the bottom.

Ok, this would make a nice background for something, so how do I now put something else as foreground?

I'll try mix again with a circle and put it on top of the sky. 

vec3 sky=mix( vec3(.0,.0,.5), vec3(.0,.5,1.), c.y*.5+.5); 
float circle=smoothstep(.25,.3,1.-length(c.xy)); 
f=vec4(mix( sky, vec3(0.6), circle),1.); 

 
Notice how I assigned the circle to a float to use as the blend value between the background sky colour, and the light grey I chose for the circle. The smoothstep made sure they were blended smoothly at the edges of the circle.

I can also use mix to take things away: 

vec3 sky=mix( vec3(.0,.0,.5), vec3(.0,.5,1.), c.y*.5+.5); 
float circle=smoothstep(.25,.3,1.-length(c.xy)); 
float shadow=smoothstep(.25,.3,1.-length(c.xy+vec2(.5,0.))); 
f=vec4(mix( sky, vec3(.6),mix(circle,0.,shadow)),1.); 

 
Now I made another copy of the circle, the same size, but shifted to the left by 0.5, called shadow. Then I used shadow as the blend value for another mix, selecting the background sky instead of the grey wherever the shadow is. This ends up taking a big chunk out of the original circle making it look a bit more moon-like, as I intended.

As you can see, mix allow much more complex images to be built up by different combinations of shapes and colour.

One interesting consequence of this way of image construction is that every pixel must calculate all the values it could possibly have - all the paths of the program flow - selecting the final one based on the mixing values.

This might seem rather expensive, and you might be wondering why I'm not making use of conditional statement such as if.

It is true that shaders support if, but it's worth remembering that usually shaders are not run on normal processors, but rather on GPUs which are made up from very many simple processors called shader units.

Very often these processors work quite inefficiently when they have to choose between two different program flows - pipelines can be stalled, and many cycles of processing time wasted.

By just calculating every path and selecting the final output with mix - which is not a conditional statement - the execution time is predictable and no time is wasted.

This is not to say a conditional version might not be faster in some cases on some GPUs. But it won't always automatically be the case.

Another approach for combining is the traditional approach to have more than one shader, and draw each item with a seperate OpenGL draw operation. In some cases this might deliver much better performance.

But it also limits the kind of combining operations that can be done because the only per-pixel data you can easily pass between the shader instances is an alpha value. It can be more complex to set up than a single shader, and is less easy to reuse in different ways than a single shader program.

Shade from the clouds
Last updated: Fri, 26 Oct 2012 06:00:00 GMT
TweetGoogle+
Since cloud services are hot, I figured that ThNdl should have one too.

Yep, you guessed it: CloudShading.

No, I'm only joking. Well, kind of.

In fact a few of you have been using one version of my "CloudShading Service" already through ThNdl Shader Editor. That is (IMVHO) quite useful for learning something about shaders. But to be honest, it's not very useful for anything practical.

However, you might have noticed that when you press the "Image" button, your browser downloads an image from a URI that seems to include the shader itself (slightly encoded of course) plus a few other parameters.

That's how the backend for the shader editor, and the serving of all the images on ThNdl works.

Behind my web server is a python server which takes shaders encoded in URIs, and passes them to a Mesa LLVMPIPE instance (which does GLES2 SW multi-threaded rendering, no X dependencies or real GPU!).

That compiles the shaders (returning any errors), renders them (to a pbuffer), encodes the image to either JPEG, PNG or GIF format, caches the result, and returns it to you, or rather your web browser.

Actually, given that it's all CPU-based, it works surprisingly well and fast. My server is a Linode 512 instance which has 4 (err...virtual) cores and 512Mb of RAM, and the rendering makes full use of all cores. It can manage something like 10 unique 512x512 shaders per second, depending on shader complexity.

However, apart from when you are editing shaders, most shaders and images requested have already been made earlier and are returned directly from the cache. This can handle a few thousand requests per second.

It all sounds a bit crazy, I know. But the practical result is that I can embed shaders into HTML and CSS wherever an Image is specified by URI. That means at least IMG elements, CSS background-image and CSS border-image.

Since images-from-shaders don't need to be prepared in advance, I can rapidly design and tweak images from the same source as the text of the blog.

I know that CSS is quite flexible, and getting more so all the time. But the potential of programmable shaders goes way beyond what "fixed function" CSS definitions can achieve.

Actually I'm hoping that the "CSS Custom filters" that Adobe has been trying to get standardised eventually bring shaders natively. But even if that comes, it will take some time before it works widely. In the meantime, my approach works practically everywhere.

Ok, so the "service" is there, has already "proven" itself by serving up 20000 or so images without problems, and maybe one or two of you are now thinking you could make use of it from some app or page of your own.

Well, feel free.

Naturally since you aren't paying anything for it, it comes with zero guarantees. But it should work from other domains, and if it doesn't I can add any needed CORS headers to the server output.

If, in the unlikely event that it suddenly gets too popular, I might pull the plug, or limit it to use only by ThNdl, or add ads in every 100th image, or (gasp) charge money, or something. But I'm not really expecting that.

Sadly, textures are not supported, even though I know that would open up huge possibilities (e.g. if you could give a URI to another image to feed in as a texture). But I'm not confortable with all the implications of that for a non-commercial service. Sorry!

Oh, so now you want some documentation too! grumble...

Ok, here goes. Requests should go to:

http://thndl.com/gl.s? width : height : format : uniforms : vertex_shader : fragment_shader

Width and height must be between 1 and 512. GIFs are limited to 128.

Format is a horrible hack. 100-0 means JPEG of that quality. -1 means PNG. -2 to -40 means GIF with that many frames.

The vertex shader is optional - I'll explain how that works later. If you don't need it, leave it and one ":" out.

Uniforms are not normally used, but up to 4 floating point values can be passed, seperated by ","

Example: 
http://thndl.com/gl.s?128:128:-1::f=vec4(1.,0.,1.,abs(c.y))

This requests a PNG 128x128 pixels, purple, transparent in the vertical middle. Here it is:  

Have fun!

The Art of Repepetition
Last updated: Mon, 22 Oct 2012 04:00:00 GMT
TweetGoogle+

(If you're reading this in the RSS feed and there isn't a wide animation above this line, I suggest you read the article from the web version for the full giftastic experience)

Sometimes, differences everywhere aren't what you need. Constraints, like limited memory, bandwidth or performance, call for a different strategy.

Filling large areas of space when you can't pick a unique colour for each point means mastering the art of repetition.

You're probably already thinking I'm talking about shaders again. But I could just as well be referring to curtains, or wallpaper, a century ago, when the art of creating beautiful seamlessly-repeating patterns was (re)discovered.

Ok, ok, so this is going to be more shader stuff after all. I'm going to describe how to work with small canvases that get presented as tiles, for example with the CSS background-image property.

But before we dive in, some news. ThNdl Shader Editor has undergone a major upgrade to support this article. Naturally it needed tiling support if we're going to be playing with tiles. But there's more.

I added a gallery which you can use to browse existing shaders, and where you can publish your creations to share with a short URL, or for others to find and play with. The Image button gives you your creation as an image file you can save easily.

And finally, it's gone boldly into the 4th dimension (i.e. time - or you might say time travel back to the 90s) with support for the venerable, but (for some reason) not easily replaced, Animated GIF format. More about that one later.

Ok, back to the topic. Tiles. Well, tiles are of course rectangular or square. Since opposite edges get drawn next to other, it is of course easy to make tiled patterns with sharp lines at the edges. For example:

 
f=vec4(c.xy*.5+.5,c.x*c.y,1.)
Easy, but...err..boring and very obviously repeating. Clearly the first thing to do is work out how to make it seamless.

One approach is to make it symmetrical within the tile about one axis so that opposite edges match. For example:

 
vec2 m=abs(c.xy);
f=vec4(m.xy,m.x*m.y,1.)
Here we used abs to reflect the negative regions of c.xy, making it symmetrical around the centre. The result is smoother and a little nicer, but the edges are still quite visible.

If we want to get rid of the edges, we're going to need to make friends with mod, one of the two masters of repetition (the other is sin).

If you don't know mod, it's simply the remainer from a division operation. In shaders, the mod function takes two values, and calculates the remainer of the first divided by the second.

The reason this is useful for us is that mod lets us do maths and pretend the edges are not there, as long as we promise to make our results at the top and bottom of the mod range match up. Take a look:

 
vec2 m=mod(c.xy+c.x+c.y,1.);
f=vec4(m.xy,m.x*m.y,1.)
First we skewed the original coordinate space, then we wrapped that in mod(1.0). That wouldn't have worked without the mod, but with the mod, the deformations on either side of the tile match up.

One thing, you can't do just any deformation - it needs to be compatible with the tile size to avoid creating breaks, which typically means integer multiples.

In this case we left the hard edges in, rather than matching them, but even now those edges are no longer identical to the tile shape. Tiling angled lines are just a short (smooth)step or two away.

 
vec2 m=mod(c.xy+c.x+c.y,1.);
f=vec4(smoothstep(.75,.5,m.y)*
smoothstep(.25,.5,m.y))
Alternatively, to get rid of lines, we could use our old friend length.

 
vec2 m=mod(c.xy+c.x+c.y,2.)-1.;
f=vec4(length(m))
Now this starts to get interesting. You might have spotted something else here - one of my favourite cheap tricks. Instead of mod(1.0), this used mod(2.0) and subtracted 1 from the result.

That puts this back into the range (-1,1), which is needed for symmetrical operations like creating a circle using length. Wrapping that in abs would create a zig-zag between 0-1 with no breaks - also very useful.

Time for something completely different... GIFs, GIFs, gimme more GIFs...

I promised to say more about GIFs in the ThNdl Shader Editor. If you click "JPEG" it will change to "GIF", and your world will go monochrome... huh?

Yeah, well, GIF, being so old, can only handle 256 colours. So, to avoid the need for dithering we only support B&W using the alpha value from the shader.

In return for this we get up to 40 frames of 10FPS animation (with quite miserable compression) that animates in most browsers. Good deal, huh?

The only other thing you need to do is use the value t in your shader. This is 0 in the first frame, and 1 in the last frame, smoothly changing in-between. You use it to drive your animation.

The interesting thing is that many of the rules that apply in 2D space for tiling, also apply to the animation tilling in the time dimension, if you want to get smooth animation that is.

Time for some examples:

 
vec2 m=mod(c.xy+c.x+c.y+2.*t,2.)-1.;
f.a=length(m)
By adding 2.*t to the deformation from the previous shader, now it scrolls smoothly forever, muhahaha.

 
float N=7.;float a=atan(c.x,c.y)+6.28319*t/N;
float b=6.28319/N;f.a=smoothstep(.5,.55,
cos(floor(.5+a/b)*b-a)*length(c.xy))
Making the heptagons slowly rotate just needs t scaled by 2/N PI and added to the angle. It only needs to turn by one segment in 40 frames so it is very smooth.

Also to be found in the gallery is the ball animation from the top of this article, and the eye-bending Riley.

Go on, have a play with it and try out some of the repetition techniques. If you come up with something nice, don't forget to post it to the gallery for the rest of us to see.

Riley
Last updated: Thu, 18 Oct 2012 20:15:00 GMT
TweetGoogle+
I was busy mixing some shaders with a bit of new server infrastructure, and something potent emerged that demanded a mini post for itself.

But first a warning. If you have eyes and a brain and you click this link, one or both of them may come away hurting. Don't say I didn't warn you!

Ok, here goes. Click the link:

Riley
Square shaped shaders
Last updated: Tue, 16 Oct 2012 09:42:00 GMT
TweetGoogle+
(Don't forget - you can click on any of the examples here to try them out in an interactive shader editor.)

We already saw how the GLSL (that's the OpenGL Shader Language) built-in function "length" can be used to calculate the distance of a point from the centre (0,0) - great for making circles.

GLSL has many other built in functions. Lets use a couple of those to make squarish shapes.

 
vec2 r=abs(c.xy);
f=vec4(r,0.,1.)
The abs function makes negative numbers positive - it reflects them around zero. Since our c coordinate is negative to the left and top, taking the abs of it and setting the red to c.x, and green to c.y, gives a symmetrical image around the centre.

 
vec2 r=abs(c.xy);
float s=max(r.x,r.y);
f=vec4(vec3(s),1.)
Now we inserted a max function. This takes two arguments, and returns the largest one for each element. In this case it returns either r.x or r.y, depending on which is larger.

This gives us an interesting and useful distance field. In this field, the value gives the distance of that point from the nearest point on the nearest edge of a square.

The simple length(c.xy) distance field we made previously was useful for giving us different sized circles. This one can give us different kinds of squares.

 
vec2 r=abs(c.xy);
float s=step(.5,max(r.x,r.y));
f=vec4(vec3(s),1.)
As with the circular distance field, a step function changes the field to a solid square shape. By changing the threshold value for the step, we can make larger or smaller squares.

But we don't have to limit ourselves to solid squares.

 
vec2 r=abs(c.xy);
float s=max(r.x,r.y);
f=vec4(vec3(step(.4,s)* step(s,.5)),1.)
By multiplying two step functions with reversed argument orders and different thresholds, we can make a hard edge which is as wide as we like.

Now if we change from using step to using smoothstep, we can make the edge softer.

 
vec2 r=abs(c.xy);
float s=max(r.x,r.y);
f=vec4(vec3(smoothstep(.3,.4,s)* smoothstep(.6,.5,s)),1.)
If you are looking for something slightly less square, then we need a different distance field, like this one.

This time we subtract an offset - in this case 0.3 - from the abs, and then take the max of that and zero before taking the length of the result.

 
vec2 r=length(max( abs(c.xy)-.3,0.));
f=vec4(r,r,r,1.)
As you can see, this again gives a square in the middle with size determined by the offset amount. But now the field near the corners is curved instead of straight.

You can probably guess already what shape we will get when we apply a step function.

 
vec2 r=step(.2, length(max(abs(c.xy)-.3,0.)));
f=vec4(r,r,r,1.)
Yes, it's a square with rounded corners. By tweaking the step threshold you can get more or less roundedness. The pair of steps from earlier could also be used to give a round edged border.

Finally lets try yet another way to create a square. This one is a bit more expensive because it's based on angles. First we find the angle of each point using the atan function.

Then we scale the angle to be in the range -1.5 to 2.5 and then round it down to the nearest integer using the floor function. This puts each point into one of four quadrants.

We then turn that back into an angle and subtract the difference between that and the actual angle. Taking cos of that difference and multiplying by the length of the original point gives the distance of that point along the vector for the centre line of the quadrant. Finally a step turns that into a square. Phew.

 
float a=atan(c.x,c.y); 
f=vec4(step(.5,cos( floor(a*.636+.5)* 1.57-a)*length(c.xy)))
The nice thing about this approach is that with a different scale factor, it can give us ANY regular polygon, not just a square. It's also easy to rotate with an offset to the angle.

Here is a parameterised version (from @srodal) which defaults to a 7-side heptagon.

 
int N=7; 
float a=atan(c.x,c.y)+.2; 
float b=6.28319/float(N); 
f=vec4(vec3(smoothstep(.5,.51, cos(floor(.5+a/b)*b-a)*length(c.xy))),1.);
These basic techniques start to open up all kinds of possibilities for constructing more interesting and useful images just using shaders.

Learning to shade
Last updated: Wed, 10 Oct 2012 14:30:00 GMT
TweetGoogle+
So far on ThNdl, I've shown you some simple OpenGL shaders, and teased you with some not-so-simple ones.

If the whole idea of shaders is new to you, you might be wondering how you can learn more about them. I think, as with most things, the best way to learn is by doing.

When I started thinking of writing about shaders, I wanted to find a way for you to play around with the ideas I am describing wherever you are.

I've spent a little time working on one solution to this, so for the coming posts I'll be linking all the shader examples to a live shader editor you can use from many recent web browsers. It doesn't support any animation, only still images, but it should still be enough for you to try all the things out.

To kick things off, you can try it out on this nice colourful shader, with just two expressions, which builds on the circle from The Pixel Swarm article.

Press or Click on it to open the interactive editor. Change some numbers, and see what happens. If you aren't sure what's going on when you get there, then just come back here and I'll give you some pointers.

 
float r=1.-length(2.*c.xy);
f=vec4(c.xy*r,r,1.)
Welcome back!

You may have noticed already that when you write a number, it needs to have a decimal point, but you can drop leading or trailing zeroes. For example: 1.0 can be written 1. and 0.1 can be written .1

New variables can be declared when you need to use a value more than once. In this case we declared a single floating point variable called r, and then used the value of r twice in the second expression.

Expressions need to end with a semi-colon, but I add one to the final expression so you don't need to do that every time.

The whole program is run once for every pixel in the image, each time with a slightly different value for the two variables I picked as the input coordinates, c.x and c.y. That means that if you want to make something other than a solid colour, you need to find ingenious mathematical ways to modify those before turning them into colours.

In the coming posts, I'll describe some of these techniques so that we can make some more complex images.

The editor I'm using does not require any new browser features like WebGL. However if you have a good PC and the right browser, you might have WebGL support. That allows you to use OpenGL and shaders from Javascript in your browser. If that's the case, here are another couple of options to try.

The first one to check is Inigo Quilez' nice Shader Toy. This comes with many examples and demos, though unfortunately some of them no longer work with the latest WebGL versions as the spec has changed since the site was first created.

Another one I like is the GLSL Sandbox by Mr Doob and friends. This has an ever expanding gallery of contributions which are fascinating to try and understand. Watch out though, many of them need a good GPU to run with reasonable speed!

On mobile devices, your options are more limited. For some mysterious reason, WebGL has been mostly missing-in-action on mobile devices. (This is odd because WebGL was based on OpenGL ES2, which is available on just about every smartphone and tablet today).

On iOS it's only available for ads (?!?), though Firefox beta for Android has recently added support. However, WebGL pages intended for high-end desktop machine will generally not work well on mobile GPUs.

Another option is to find a port of the Shader Toy as an app, e.g. from the Android Play Store.

Now there is nothing to stop you learning to shade!

Whoops, we forgot a dimension
Last updated: Mon, 08 Oct 2012 09:42:00 GMT
TweetGoogle+
Many of you are probably reading this on a device that looks something like this. 

The "tall thin slab full of screen" form factor for phones seems to be becoming the norm as we move away from hardware keys. The biggest variable these days seems to be size, which is quite proportional to price.

Others of you will have a bigger device - some kind of tablet - looking something like this. 

The common factor with these two classes of device is probably that they have large capacitive touch screens, and no keyboard.

A few years ago when people started to seriously try and use the web from devices other than PCs, sites were all "Best Viewed at 1024x768" or even "800x600".

Squeezing those sites onto small portrait-first screens was a challenge for mobile browsers. Zooming in to make text readable often gave columns that were too wide to fit, and panning round to find everything was a frustrating experience.

This was solved eventually by the emergence of special layouts and sites for mobile devices, that squeezed the site to one screen width of readable text, and as many pages high as needed to show everything. Now that paradigm already seems ubiquitous.

But during that shift, we forgot a dimension. Now we are all happily flicking up and down, going forwards and backwards through our giant articles and lists. But what about the x dimension - left and right?

When I started thinking about how ThNdl could look and be used on a portrait touchscreen phone, I wanted to find a way for effortless navigation both within and between articles.

I wanted to avoid the need for you to have to find some button to press to change article. And I wanted to avoid something involving layers of content that moved independantly, because making that kind of design work everywhere with good performance is a challenge.

While exploring the options, I found out support for CSS columns already seems to be widespread in most current browsers. That allows you to display items split over a specified number of columns, like this:

div{
-column-count: 2;
}

The resulting item will be as high as needed to show all the content. Unfortunately however, splitting a long article like that makes things worse. Now when you get to the bottom of the first column, you need to scroll all the way back up to the top and right to continue reading.

However, CSS columns can also be set to automatically add as many columns as needed to display all the content. All you have to do is specify the height. And of course the height to use on a mobile device should be the screen height, so that now you only need to scroll right to continue reading.

As long as the columns are taller than wider, this is theoretically easier to use than the single column view, because you have to scroll a shorter distance to get the next lump of new text fully visible.

And when column support isn't available (*cough*IE*cough*), it falls back gracefully to a single column.

So columns seemed viable for articles, and also quite natural since people have been reading articles that way on paper for a long time. The x dimension was claimed for navigating within articles. That left the y dimension - up and down - free for browsing between articles, with just one screen-height of scrolling needed.

With the articles all aligned at the left, a quick flick left should take the reader quickly from the end of an article back to column containing the start or all articles - much more quickly than doing that same in a single vertical column.

Conceptually it looked something like this. 

So much for the neat theory... as always, the practical reality quickly gets messy and complicated.

With some javascript to deal with the usual browser differences, a resizing function got the basics in place. On a portrait phone screen you see one screen height column. Panning right takes you to the next column.

On larger screens, the resizing tries to avoid too wide or too thin columns, and chooses either 2 or 3. For instance, on an iPad you should see 2 columns in portrait, and 3 in landscape.

The resize gets triggered by a window resize, which happens on an orientation change in most environments.

Now, text is quite breakable - it doesn't mind too much what column it's in. But images are a different story. The CSS column model can be persuaded to try not to break items in the middle, which works on some more recent browsers. But it leaves a big gap.

If the column height gets smaller than the biggest image, then all bets are off, and the images will get broken in an ugly way.

Unfortunately, landscape phone screens turn out to be the case where the screen height is very squeezed due to system and (in Chrome's case) browser UI taking up space at the top.

Another problem was the breaks between articles. When the CSS model expands columns, it doesn't seem to give you a way to find out how many columns it actually made, or how wide the result really is. So the breaks were either too wide or too thin. I had to workaround that by checking different values in different browsers.

What about usability? One big advantage of normal one dimensional scrolling is that you can be very sloppy when flicking - it only moves up or down anyway. But when the page is wider than the screen, sloppy flicking takes you away from the article. Mobile Safari tries to constrain to either horizontal or vertical scrolling, which helps, but it stays stuck in one mode until scrolling stops which can be annoying.

And then there is the good old desktop. Mac users and some PC touchpad users have it ok with horizontal and vertical two-finger scroll gestures, but in other places horizontal scrolling is a pain. Dragging scroll bars with a mouse pointer is also a bit hopeless.

Keyboard scrolling works, but it's often jerky, and with speed based on key repeat intervals. And there is no Page-Left, Page-Right key to help (which is a bit odd when you think about pages in books!).

I tried Javascript tricks liks snapping from onscroll after a little while, or overriding cursor keys, but so far they have always hit compatibility problems in different places

Regarding Javascript, ThNdl web site is currently "App like" - which means the code you first get from the server contains none of the articles - they are fetched afterwards with Javascript. So, no JS, no articles - it turns out that's a problem for some users, and other sites showing summaries (like LinkedIn).

So, what next? Well, I introduced a full-content RSS feed as an alternative (1D, no JS) way to read, because many people just don't want to have to read with a browser, full stop.

The landscape phone layout has been tweaked to use a smaller font which should improve it. And I'm going to see if I can do something to help desktop/mouse/keyboard usability - for instance some keyboard shortcuts. After that, we'll see.

But I'm not too keen to have to learn to forget that x dimension again.

Logo
Last updated: Thu, 04 Oct 2012 09:42:00 GMT
TweetGoogle+
In pixels


In code

c=vec4(pos.xy,0.,0.); b=vec4(pos.x*cos(o.x)-pos.y*sin(o.x), pos.x*sin(o.x)+pos.y*cos(o.x),0.,0.); 
float r,s,l,h,i,k,m,n; m=clamp(1.5-abs(b.y),0.2,1.5)+d.x*50.; i=step(-0.01,c.y); h=step(-0.01,b.y); k=step(-0.99,b.y)*(1.-h)*(1.-smoothstep(0.027*m-d.x*2., 0.03*m,abs(b.x))); l=length(c); r=1.0-smoothstep(1.-d.x*2.,1.,l); s=1.0-smoothstep(.5-d.x*2.,0.5,l); float t=atan(c.x/c.y); float u=(t+3.141*0.5)/3.141; vec4 ryg=vec4(0.69*clamp(mix(vec3(.0,2.,0.), vec3(2.,0.,0.),u),0.0,1.0),2.4-2.*l); vec4 tg=mix(vec4(ryg.rgb,0.),ryg,r); n=clamp(1.75*l-0.75,0.,1.); vec4 bg=mix(tg,vec4(0.,0.,0.,1.-n),i); float v=atan(c.x,c.y); n=.9+.1*sin((20.*l+v)*5.); vec4 w=vec4(n,n,n,1.); vec4 e=mix(bg,w*vec4(vec3(abs(mod(v+3.141*0.75, 2.*3.141)/3.141-1.)),1.),s); f=mix(e,vec4(vec3(1.-(30.*abs((b.x/m+0.01)))),1.),k);

The Pixel Swarm
Last updated: Wed, 03 Oct 2012 09:42:00 GMT
TweetGoogle+
Until quite recently, I was working a lot with Qt. One of the best and most fun features in the latest version - Qt5 - is the QML ShaderEffect, which lets you easily draw and animate almost anything to the screen using just a tiny OpenGL shader program.

If you aren't familiar with shaders, they are small programs - typically only a few lines of code - which are run directly on a GPU. But, be warned... once you start learning and writing them it's hard to stop!

(When I first heard about shaders, I have to admit they sounded like scary arcane magic that I would never be able get to grips with. But it turned out that the magic was mostly just boilerplate setup code you needed to get started, and once you got past that they were actually quite simple.)

In the dim distant past of software history (say, 5 years ago) it has been common to create complex graphic scenes by making many calls for drawing primitive shapes on top of each other, giving positions, sizes and colours each time. These were the GDI or (in Qt's case) Painter APIs.

However, when you want to get the most out of that shiny GPU in your computer or phone, this is not always the best way to do things. Every time you touch a pixel, it potentially has to run another program.

If you have many items on top of each other, this means the hardware has to change state very often - something GPUs really don't like to do - and in the worst case will do a lot of work which is never even seen if the shape on top is not transparent.

An alternative to that traditional approach is to create a single program for the whole scene which will result in the desired image emerging after it is run just once, without any expensive state changes, for every pixel.

It's like this: imagine a swarm of exotic chameleon bees buzzing in front of you.

Through millions of years of genetic programming (if you believe in that sort of thing), each bee has been programmed to change colour depending only on how high up it is in the air.

Near the ground, each bee makes itself more brown, and in the air it makes itself more blue. The bees cannot communicate with each other, and yet this is what you see: 
Probably not so surprising when described that way.

Now lets take that idea of a swarm of independant pixels and start to program them into something interesting. We'll start with a simple GLSL shader that takes an interpolated 2D (x and y) coordinate "c" which is (-1,-1) in the top left, and smoothly changes to (+1,+1) at the bottom right. I'll leave out the boilerplate to keep things clearer (f is the same as gl_FragColor). 

f = vec4(c.x, c.y, 0.0, 1.0);

f expects 4 values - red, green, blue and alpha (transparency), each in the range 0 to 1, and collected into a "vec4". It then sets the pixel to that colour.

This program just sets the red and green components of each pixel to the coordinate. Since these colour values can only go from 0 to 1, and not -1 to +1, that makes the top left region (with all negative coordinates) all black, and the bottom right corner yellow, like this: 
Now lets try something more complex: 

float d = length(c.xy); 
f = vec4(d, d, d, 1.0);

This time we've created another variable d, and set that to the distance of every pixel from the centre using the built in "length" function. Then we set the colour to that value. The result is a circular gradient, black in the centre, and white at the edges, like this: 
Ok, so we've now done linear and radial gradients with just 3 lines of code. From here, it's just a few steps to go from a gradient, to a shape. 

float d = step(0.1, 1.0-length(c.xy)); 
f = vec4(0.0, 1.0, 0.0, d);

First we subtract the length from 1, so that it is 1 at the centre and smaller as it gets further away.

Then we add a step function. This returns either 0 or 1 depending on whether the argument is above or below the threshold (0.1 in this case).

Finally, we set the green colour value to 1, and instead change only the final "alpha" or transparency value. Together, this gives us a green circle with a transparent background: 
Remember, none of the bees, err, I mean pixels, have communicated with each other to make the circle. It just emerged by running the same program for each pixel, telling each one only where it is in the picture.

That's important for performance, because it means this program can be run in parallel on many independant processors (modern desktop GPUs have hundreds of them), without each one needing to wait to know the result of any of the other pixels.

Sadly the edges of the circle that emerged are horribly jagged, and I really hate the jaggies. But we can fix that with another small change: 

float d = smoothstep(0.08,0.1,1.-length(c.xy)); 
f = vec4(0.0, 1.0, 0.0, d);

The smoothstep function is almost like step, but takes 2 thresholds. Below the lower it is zero, and above the upper it is one. But in between it smoothly interpolates between 0 and 1. Now the jaggies are gone: 
Mmmm, smoooth.

Well, I think that should be just enough for now to start to whet your appetite for shaders.

Welcome to ThNdl
Last updated: Tue, 02 Oct 2012 09:42:00 GMT
TweetGoogle+
Welcome!

The first rule of ThNdl is don't forget to PAN RIGHT to find the articles (unless you're on IE with no columns support).

This is a blog written by myself, Andrew Baldwin (baldand@twitter).

The articles will cover all kinds of things I'm interested in, including but not limited to software performance, graphics, mobile devices, and user experience.

ThNdl is a reboot of a similar blog - TheNeedle - I used to write a few years ago for the Nokia intranet BlogHub, so if you are a former or (like me) current Nokian, it might seem vaguely familiar. However, this incarnaion of the blog won't be focusing on Nokia.

In classic Internet tradition, during the rebooting process I felt compelled to lose the vowels in order to grab the shortest relevant domain name that didn't already point to a knitting site or domain reseller. Actually, I was a bit surprised to find a suitable 5 letter .com still available in 2012.

Unlike another recent BlogHub reboot - @jpzip's realboxscore.com - I decided not to use an existing blog platform to start with, but rather to learn through mistakes by rolling my own. So, expect the unexpected, and give me feedback, as you read this on hardware and platforms I don't have. We'll see how this works out, especially when it comes to comments.

Regarding comments and feedback, until I get round to adding a comment system, you can send your thoughts directly to me (once again, @baldand) via Twitter.

I've been told that columns are old fashioned, so you might be wondering about the layout. I'm already a bit bored and frustrated with the "giant scrolling 1D page" paradigm when reading on mobile devices.

So, at least to start with, I'm trying an experiment with 2D layout. On most semi-modern browsers, articles should be screen height and as many columns as needed to the right. To jump to the next articles, just scroll up or down. Let me know what you think of that, or if I should just give in and go 1D.

Finally, apologies in advance if you happen to come from across the pond, but colourful British-English spellings - notable for a lack of zees, or even zeds - shall be utilised throughout. I'm sure you'll get used to it in case you carry on reading!

All content Â©2012 Andrew Baldwin. All reproduction/syndication rights reserved. Made with vim,git,python,mesa and GLSL.
