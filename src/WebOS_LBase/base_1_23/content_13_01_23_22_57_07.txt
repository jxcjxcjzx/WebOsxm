

leftnoteasy
 
关注于 机器学习、数据挖掘、并行计算、数学
 



机器学习中的算法(2)-支持向量机(SVM)基础



版权声明：

    本文由LeftNotEasy发布于http://leftnoteasy.cnblogs.com, 本文可以被全部的转载或者部分使用，但请注明出处，如果有问题，请联系wheeleast@gmail.com

 

前言：

    又有很长的一段时间没有更新博客了，距离上次更新已经有两个月的时间了。其中一个很大的原因是，不知道写什么好-_-，最近一段时间看了看关于SVM(Support Vector Machine)的文章，觉得SVM是一个非常有趣，而且自成一派的方向，所以今天准备写一篇关于关于SVM的文章。

    关于SVM的论文、书籍都非常的多，引用强哥的话“SVM是让应用数学家真正得到应用的一种算法”。SVM对于大部分的普通人来说，要完全理解其中的数学是非常困难的，所以要让这些普通人理解，得要把里面的数学知识用简单的语言去讲解才行。而且想明白了这些数学，对学习其他的内容也是大有裨益的。我就是属于绝大多数的普通人，为了看明白SVM，看了不少的资料，这里把我的心得分享分享。

    其实现在能够找到的，关于SVM的中文资料已经不少了，不过个人觉得，每个人的理解都不太一样，所以还是决定写一写，一些雷同的地方肯定是不可避免的，不过还是希望能够写出一点与别人不一样的地方吧。另外本文准备不谈太多的数学（因为很多文章都谈过了），尽量简单地给出结论，就像题目一样-机器学习中的算法（之前叫做机器学习中的数学），所以本系列的内容将更偏重应用一些。如果想看更详细的数学解释，可以看看参考文献中的资料。



一、线性分类器：

    首先给出一个非常非常简单的分类问题（线性可分），我们要用一条直线，将下图中黑色的点和白色的点分开，很显然，图上的这条直线就是我们要求的直线之一（可以有无数条这样的直线）

    假如说，我们令黑色的点 = -1， 白色的点 =  +1，直线f(x) = w.x + b，这儿的x、w是向量，其实写成这种形式也是等价的f(x) = w1x1 + w2x2 … + wnxn + b, 当向量x的维度=2的时候，f(x) 表示二维空间中的一条直线， 当x的维度=3的时候，f(x) 表示3维空间中的一个平面，当x的维度=n > 3的时候，表示n维空间中的n-1维超平面。这些都是比较基础的内容，如果不太清楚，可能需要复习一下微积分、线性代数的内容。

    刚刚说了，我们令黑色白色两类的点分别为+1, -1，所以当有一个新的点x需要预测属于哪个分类的时候，我们用sgn(f(x))，就可以预测了，sgn表示符号函数，当f(x) > 0的时候，sgn(f(x)) = +1, 当f(x) < 0的时候sgn(f(x)) = –1。

    但是，我们怎样才能取得一个最优的划分直线f(x)呢？下图的直线表示几条可能的f(x)



    一个很直观的感受是，让这条直线到给定样本中最近的点最远，这句话读起来比较拗口，下面给出几个图，来说明一下：

    第一种分法：

 

    第二种分法：



    这两种分法哪种更好呢？从直观上来说，就是分割的间隙越大越好，把两个类别的点分得越开越好。就像我们平时判断一个人是男还是女，就是很难出现分错的情况，这就是男、女两个类别之间的间隙非常的大导致的，让我们可以更准确的进行分类。在SVM中，称为Maximum Marginal，是SVM的一个理论基础之一。选择使得间隙最大的函数作为分割平面是由很多道理的，比如说从概率的角度上来说，就是使得置信度最小的点置信度最大（听起来很拗口），从实践的角度来说，这样的效果非常好，等等。这里就不展开讲，作为一个结论就ok了，:)

    上图被红色和蓝色的线圈出来的点就是所谓的支持向量(support vector)。

       上图就是一个对之前说的类别中的间隙的一个描述。Classifier Boundary就是f(x)，红色和蓝色的线（plus plane与minus plane）就是support vector所在的面，红色、蓝色线之间的间隙就是我们要最大化的分类间的间隙。

    这里直接给出M的式子：（从高中的解析几何就可以很容易的得到了，也可以参考后面Moore的ppt）



    另外支持向量位于wx + b = 1与wx + b = -1的直线上，我们在前面乘上一个该点所属的类别y（还记得吗?y不是+1就是-1），就可以得到支持向量的表达式为：y(wx + b) = 1，这样就可以更简单的将支持向量表示出来了。

    当支持向量确定下来的时候，分割函数就确定下来了，两个问题是等价的。得到支持向量，还有一个作用是，让支持向量后方那些点就不用参与计算了。这点在后面将会更详细的讲讲。

    在这个小节的最后，给出我们要优化求解的表达式：



    ||w||的意思是w的二范数，跟上面的M表达式的分母是一个意思，之前得到，M = 2 / ||w||，最大化这个式子等价于最小化||w||, 另外由于||w||是一个单调函数，我们可以对其加入平方，和前面的系数，熟悉的同学应该很容易就看出来了，这个式子是为了方便求导。

    这个式子有还有一些限制条件，完整的写下来，应该是这样的：（原问题）



    s.t的意思是subject to，也就是在后面这个限制条件下的意思，这个词在svm的论文里面非常容易见到。这个其实是一个带约束的二次规划(quadratic programming, QP)问题，是一个凸问题，凸问题就是指的不会有局部最优解，可以想象一个漏斗，不管我们开始的时候将一个小球放在漏斗的什么位置，这个小球最终一定可以掉出漏斗，也就是得到全局最优解。s.t.后面的限制条件可以看做是一个凸多面体，我们要做的就是在这个凸多面体中找到最优解。这些问题这里不展开，因为展开的话，一本书也写不完。如果有疑问请看看wikipedia。



二、转化为对偶问题，并优化求解:

    这个优化问题可以用拉格朗日乘子法去解，使用了KKT条件的理论，这里直接作出这个式子的拉格朗日目标函数：



    求解这个式子的过程需要拉格朗日对偶性的相关知识（另外pluskid也有一篇文章专门讲这个问题），并且有一定的公式推导，如果不感兴趣，可以直接跳到后面用蓝色公式表示的结论，该部分推导主要参考自plukids的文章。

    首先让L关于w，b最小化，分别令L关于w，b的偏导数为0，得到关于原问题的一个表达式



    将两式带回L(w,b,a)得到对偶问题的表达式

 

    新问题加上其限制条件是（对偶问题）:



    这个就是我们需要最终优化的式子。至此，得到了线性可分问题的优化式子。

    求解这个式子，有很多的方法，比如SMO等等，个人认为，求解这样的一个带约束的凸优化问题与得到这个凸优化问题是比较独立的两件事情，所以在这篇文章中准备完全不涉及如何求解这个话题，如果之后有时间可以补上一篇文章来谈谈:)。



三、线性不可分的情况（软间隔）：

    接下来谈谈线性不可分的情况，因为线性可分这种假设实在是太有局限性了：

    下图就是一个典型的线性不可分的分类图，我们没有办法用一条直线去将其分成两个区域，每个区域只包含一种颜色的点。

     要想在这种情况下的分类器，有两种方式，一种是用曲线去将其完全分开，曲线就是一种非线性的情况，跟之后将谈到的核函数有一定的关系：

     另外一种还是用直线，不过不用去保证可分性，就是包容那些分错的情况，不过我们得加入惩罚函数，使得点分错的情况越合理越好。其实在很多时候，不是在训练的时候分类函数越完美越好，因为训练函数中有些数据本来就是噪声，可能就是在人工加上分类标签的时候加错了，如果我们在训练（学习）的时候把这些错误的点学习到了，那么模型在下次碰到这些错误情况的时候就难免出错了（假如老师给你讲课的时候，某个知识点讲错了，你还信以为真了，那么在考试的时候就难免出错）。这种学习的时候学到了“噪声”的过程就是一个过拟合（over-fitting），这在机器学习中是一个大忌，我们宁愿少学一些内容，也坚决杜绝多学一些错误的知识。还是回到主题，用直线怎么去分割线性不可分的点：

     我们可以为分错的点加上一点惩罚，对一个分错的点的惩罚函数就是这个点到其正确位置的距离：





    在上图中，蓝色、红色的直线分别为支持向量所在的边界，绿色的线为决策函数，那些紫色的线表示分错的点到其相应的决策面的距离，这样我们可以在原函数上面加上一个惩罚函数，并且带上其限制条件为：

 

    公式中蓝色的部分为在线性可分问题的基础上加上的惩罚函数部分，当xi在正确一边的时候，ε=0，R为全部的点的数目，C是一个由用户去指定的系数，表示对分错的点加入多少的惩罚，当C很大的时候，分错的点就会更少，但是过拟合的情况可能会比较严重，当C很小的时候，分错的点可能会很多，不过可能由此得到的模型也会不太正确，所以如何选择C是有很多学问的，不过在大部分情况下就是通过经验尝试得到的。

    接下来就是同样的，求解一个拉格朗日对偶问题，得到一个原问题的对偶问题的表达式：



    蓝色的部分是与线性可分的对偶问题表达式的不同之处。在线性不可分情况下得到的对偶问题，不同的地方就是α的范围从[0, +∞)，变为了[0, C]，增加的惩罚ε没有为对偶问题增加什么复杂度。



四、核函数：

    刚刚在谈不可分的情况下，提了一句，如果使用某些非线性的方法，可以得到将两个分类完美划分的曲线，比如接下来将要说的核函数。

    我们可以让空间从原本的线性空间变成一个更高维的空间，在这个高维的线性空间下，再用一个超平面进行划分。这儿举个例子，来理解一下如何利用空间的维度变得更高来帮助我们分类的（例子以及图片来自pluskid的kernel函数部分）：

    下图是一个典型的线性不可分的情况



    但是当我们把这两个类似于椭圆形的点映射到一个高维空间后，映射函数为：

    用这个函数可以将上图的平面中的点映射到一个三维空间（z1,z2,z3)，并且对映射后的坐标加以旋转之后就可以得到一个线性可分的点集了。











    用另外一个哲学例子来说：世界上本来没有两个完全一样的物体，对于所有的两个物体，我们可以通过增加维度来让他们最终有所区别，比如说两本书，从(颜色，内容)两个维度来说，可能是一样的，我们可以加上 作者 这个维度，是在不行我们还可以加入 页码，可以加入 拥有者，可以加入 购买地点，可以加入 笔记内容等等。当维度增加到无限维的时候，一定可以让任意的两个物体可分了。

    回忆刚刚得到的对偶问题表达式：

 

    我们可以将红色这个部分进行改造，令：

     这个式子所做的事情就是将线性的空间映射到高维的空间,k(x, xj)有很多种，下面是比较典型的两种：

    上面这个核称为多项式核，下面这个核称为高斯核，高斯核甚至是将原始空间映射为无穷维空间，另外核函数有一些比较好的性质，比如说不会比线性条件下增加多少额外的计算量，等等，这里也不再深入。一般对于一个问题，不同的核函数可能会带来不同的结果，一般是需要尝试来得到的。



五、一些其他的问题：

     1）如何进行多分类问题：

     上面所谈到的分类都是2分类的情况，当N分类的情况下，主要有两种方式，一种是1 vs (N – 1)一种是1 vs 1，前一种方法我们需要训练N个分类器，第i个分类器是看看是属于分类i还是属于分类i的补集（出去i的N-1个分类）。

     后一种方式我们需要训练N * (N – 1) / 2个分类器，分类器(i,j)能够判断某个点是属于i还是属于j。

     这种处理方式不仅在SVM中会用到，在很多其他的分类中也是被广泛用到，从林教授（libsvm的作者）的结论来看，1 vs 1的方式要优于1 vs (N – 1)。

     2）SVM会overfitting吗？

     SVM避免overfitting，一种是调整之前说的惩罚函数中的C，另一种其实从式子上来看，min ||w||^2这个看起来是不是很眼熟？在最小二乘法回归的时候，我们看到过这个式子，这个式子可以让函数更平滑，所以SVM是一种不太容易over-fitting的方法。



参考文档：

    主要的参考文档来自4个地方，wikipedia（在文章中已经给出了超链接了），pluskid关于SVM的博文，Andrew moore的ppt（文章中不少图片都是引用或者改自Andrew Moore的ppt，以及prml

     

 

 



分类: 机器学习, 数学
 
标签: Support Vector Machine, 支持向量机, 核函数, SVM, Kernel, Optimization, Quadratic, 二次规划
 
绿色通道： 好文要顶 关注我 收藏该文与我联系 




LeftNotEasy
 关注 - 15
 粉丝 - 296 



+加关注 


9

0


 (请您对文章做出评价) 


« 博主上一篇：机器学习中的算法(1)-决策树模型组合之随机森林与GBDT
» 博主下一篇：PyMining-开源中文文本数据挖掘平台 Ver 0.1发布
« 首页上一篇：QB虐杀游戏！2nd
» 首页下一篇：在 JavaScript 实现多播事件、属性设置/读取器

 
posted on 2011-05-02 20:56 LeftNotEasy 阅读(17126) 评论(22) 编辑 收藏


 

评论

#1楼 2011-05-03 06:26G.Anthony  


很难理解啊

支持(0)反对(0)


#2楼 2011-05-03 08:27ganggang  


以前学过支持向量机的课，但是一直处于半懂状态，看了你的指导，豁然开朗啊，谢谢。

支持(0)反对(0)


#3楼 2011-05-03 08:27ganggang  


写的非常好

支持(0)反对(0)


#4楼 2011-05-04 20:10Leo Zhang  


嗯 写的很明白 我个人是非常喜欢svm
基本上在数据维度特别大的时候 我还是比较喜欢用线性核或者径向核 如果使用sigmoid核则svm就变成了一个包含一个隐层的多层感知器 只不过隐层节点个数不用咱们自己去定且不会陷入局部最优 我觉得这点挺爽
 关于软间隔优化有一阶软间隔和二阶软间隔 可以用一个通式来表示 这样研究时候也比较方便
 目前解支持向量机的算法基本分两种 求解原问题或者求解其对偶问题 当然不同算法适合于不同场景 没有最好的只有最适合的
 对于对偶问题我想多说一点 这时候计算的复杂度不再取决于空间维数而取决于样本数（特别是支持向量个数）因此虽说feature越少越具有代表性越好 不过我不知道某些情况下是不是可以弱化特征提取的作用 换句话说我觉得svm是最不依赖于特征提取过程的分类算法
 总之svm是一种超赞的思想或者方法 嘿嘿



支持(0)反对(0)


#5楼[楼主] 2011-05-04 22:43LeftNotEasy  


@Leo Zhang
呵呵，谢谢补充，最好能放上一点关于解法的参考资料，我也可以补充到博客中来。
其实我也不准备现在把svm搞得太深，只是当做一个知识储备，所以对于一些解法就暂时不去研究了，不过我倒是有打算再好好看看libsvm的代码，我大概读了一下，结构、效率都兼顾得不错。完整的看看应该是好处挺多的。

支持(0)反对(0)


#6楼 2011-05-05 21:53Leo Zhang  


@LeftNotEasy
参考资料：
1、An Introduction to Support Vector Machines and Other Kernel-based Learning Methods（中文译本叫支持向量机导论） 
2、the nature of statistical learning theory（中文译本叫统计学习理论的本质） 这本书偏重于理论
3、A Dual Coordinate Descent Method for Large-scale Linear SVM，线性svm的一个效率很高的算法，针对的是对偶问题，思想是在不同的坐标方向上通过固定其它分量来最小化目标函数
4、Sequential minimal optimization for SVM，SMO方法
5、PSVM: Parallelizing Support Vector Machines on Distributed Computers，另外一种思路，在Grim矩阵上做文章，利用不完全乔里斯基分解来实现并行的svm
6、这个网址下有较多资料http://hi.baidu.com/zxdker/blog/item/bbe1571e625b1fcfa686692e.html

支持(0)反对(0)


#7楼[楼主] 2011-05-06 12:51LeftNotEasy  


@Leo Zhang
谢谢补充，呵呵：）

支持(0)反对(0)


#8楼 2011-05-18 00:43蓝柯  


谢谢楼主，写的深入浅出！

支持(0)反对(0)


#9楼 2011-08-23 10:19hailong  


谢谢啦，写的非常好，不过如果能看到博主把实际做的东西写出来点就更好啦！

支持(0)反对(0)


#10楼 2011-08-23 10:25hailong  


@Leo Zhang
说得好！

支持(0)反对(0)


#11楼[楼主] 2011-08-24 09:33LeftNotEasy  


@hailong
http://www.cnblogs.com/vivounicorn/ Leo Zhang的博客里面有更多关于SVM的文章，他也正在实现一个SVM，可以期待一下，呵呵～

支持(0)反对(0)


#12楼 2011-08-24 14:45hailong  


@LeftNotEasy
我也有在看啊，不过博主如果写个libsvm做实际项目的例子给大家看看就好啦啊，我现在libsvm拿到手，可是不知道具体该怎么搞啊

支持(0)反对(0)


#13楼 2011-11-02 15:59wbleach  


不知道是不是理解错了，在文章的开头的时候，怎么感觉当X是一维的时候，f(x)应该是一个二维空间里面的直线，当x是n-1维的时候，f(x)应该是n维里面的超平面呢？
求解？

支持(1)反对(0)


#14楼[楼主] 2011-11-03 09:24LeftNotEasy  


@wbleach
我的理解是，x如果是2维（平面），那么要区分这个2维的点，则需要画一条1维多平面（直线），所以f(x)的维度=x的维度-1

支持(0)反对(0)


#15楼 2011-11-05 00:40lighthearts  


还没读完，求偏导那里有一个小的标记错误（对L求b偏导那里），最近在看利用svm做回归预测（低阶映射到高阶成线性函数），想不通使用这样的优化求值（w，b）对最后的函数有什么影响。。。

支持(0)反对(0)


#16楼 2011-11-22 22:53RunZhi  


写的太好了。

发现了两个小问题：一是lighthearts提出的笔误，二是，偏导之后L的表达式的两项的前一项好像系数不是1/2，下文也没有1/2。不知道是不是我理解错了... ...


支持(0)反对(0)


#17楼[楼主] 2011-11-25 17:35LeftNotEasy  


@lighthearts
@RunZhi
谢谢二位这么仔细地指出错误，这里确实是一个标记的错误，不过好在不算太影响阅读，这里就先不改了

支持(0)反对(0)


#18楼 2011-12-23 09:58placebo  


LZ写的太棒了 超喜欢你的风格
但是机器学习系列很久没更新啦 期待下一篇啊

支持(0)反对(0)


#19楼[楼主] 2011-12-26 22:09LeftNotEasy  


@placebo
呵呵，谢谢，现在一天比一天忙，真来不及写了。。。

支持(0)反对(0)


#20楼 2012-11-01 09:00austinls  


太谢谢楼主了！非常易懂！
我弄完这个准备开始神经网络了！

支持(0)反对(0)


#21楼 2012-11-10 19:10等你爱我  


高手，顶礼膜拜！

支持(0)反对(0)


#22楼 2012-12-10 22:01DoctorXu  


开头处写错了，x为2维向量时，f(x)=wx+b代表3维空间中的平面，正确写法为f(x)=0

支持(0)反对(0)


 


刷新评论刷新页面返回顶部
 

注册用户登录后才能发表评论，请 登录 或 注册，访问网站首页。

IE10：全面支持HTML5，让你创造更多
找优秀程序员，就在博客园

 
博客园首页博问新闻闪存程序员招聘知识库
 





最新IT新闻:
 · 诺基亚今年将推真正PureView技术WP手机
 · 夏普智能手机面临激烈竞争
 · Yandex搜索应用截屏曝光 使用Nuance语音技术
 · 三星去年超苹果成最大半导体采购厂商
 · RIM发布新版移动管理软件BES 10
» 更多新闻...

最新知识库文章:

 · 浏览器的重绘与重排
 · Mac OS X 背后的故事
 · 我再也不想在任何头文件中看到"using namespace xxx;"了
 · 异常以及异常处理框架探析
 · JavaScript 项目优化总结

» 更多知识库文章... 





导航
 博客园
首页

联系
订阅
管理
 



我的标签
 
机器学习(7)
搜索引擎(7)
Lucene(6)
machine learning(4)
pymining(3)
mathmatics(3)
人工智能(3)
数据挖掘(3)
PCA(2)
Model Combining(2)
更多
 
随笔分类
Hadoop(2) 
Lucene C++重写心得(2) 
Lucene JAVA心得(9) 
安排，计划，总结(2) 
分布式存储(2) 
机器学习(7) 
结构设计(1) 
数学(7) 

随笔档案
2012年2月 (1) 
2011年9月 (1) 
2011年8月 (1) 
2011年5月 (5) 
2011年3月 (1) 
2011年2月 (1) 
2011年1月 (3) 
2010年12月 (2) 
2010年11月 (1) 
2010年10月 (1) 
2010年9月 (2) 
2010年8月 (1) 
2010年1月 (12) 
2009年12月 (2) 
2009年11月 (4) 

最新评论
 

1. Re:机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用
最近正好在做视觉方向的东西 里面用到SVD 看了你的文章收到一些启发 文章写的非常不错 赞一个...--银翼的奇术师
2. Re:机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用
第5个链接失效了，博主还能找到吗，谢谢了--宁雨
3. Re:机器学习中的算法(2)-支持向量机(SVM)基础
开头处写错了，x为2维向量时，f(x)=wx+b代表3维空间中的平面，正确写法为f(x)=0--DoctorXu
4. Re:机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用
挺好的  pca那块的解释很容易想象和理解 谢谢--丕子
5. Re:机器学习中的算法(2)-支持向量机(SVM)基础
高手，顶礼膜拜！--等你爱我
 
阅读排行榜
 

1. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用(28876)
2. 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)(19718)
3. 机器学习中的算法(2)-支持向量机(SVM)基础(17125)
4. 机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)(16009)
5. 机器学习中的算法(1)-决策树模型组合之随机森林与GBDT(14555)
 
评论排行榜
 

1. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用(25)
2. 机器学习中的算法(2)-支持向量机(SVM)基础(22)
3. 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)(21)
4. 机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)(20)
5. 贝叶斯、概率分布与机器学习(19)
 
推荐排行榜
 

1. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用(21)
2. 为什么Hadoop将一定会是分布式计算的未来？(15)
3. 机器学习中的算法(1)-决策树模型组合之随机森林与GBDT(12)
4. 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)(10)
5. 机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)(9)
 

Powered by: 博客园 Copyright © LeftNotEasy 

